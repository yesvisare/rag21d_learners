{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.1 Understanding Vector Databases\n",
    "### Vector Search Foundations for RAG\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the semantic gap and why vector databases solve it\n",
    "- Learn how embeddings represent meaning as 1536-dimensional vectors\n",
    "- Master cosine similarity calculations\n",
    "- Create and query Pinecone indexes with proper error handling\n",
    "- Apply metadata filtering and score thresholding\n",
    "- Debug common failures and prevent them\n",
    "\n",
    "**Duration:** 60-90 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Why Vector Databases?\n",
    "\n",
    "### The Semantic Gap Problem\n",
    "\n",
    "Traditional keyword search fails to understand **meaning**:\n",
    "- Query: \"climate change impacts\"\n",
    "- Misses: \"global warming effects\", \"environmental consequences of CO2 emissions\"\n",
    "\n",
    "Vector databases solve this through **semantic search** - understanding intent, not just matching keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embeddings: Meaning as Numbers\n",
    "\n",
    "Embeddings convert text ‚Üí high-dimensional numerical vectors:\n",
    "- OpenAI's `text-embedding-3-small`: **1536 dimensions**\n",
    "- Similar meanings = close together in vector space\n",
    "- Different meanings = far apart\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for similar and different sentences\n",
    "from openai import OpenAI\n",
    "import config\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
    "\n",
    "# Three sentences: two similar, one different\n",
    "sentences = [\n",
    "    \"The weather is beautiful today\",\n",
    "    \"It's a gorgeous sunny day\",\n",
    "    \"Python is a programming language\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for sentence in sentences:\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=sentence\n",
    "    )\n",
    "    embedding = response.data[0].embedding\n",
    "    embeddings.append(embedding)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Dimensions: {len(embedding)}\")\n",
    "    print(f\"First 5 values: {embedding[:5]}\")\n",
    "    print()\n",
    "\n",
    "# Expected output:\n",
    "# Dimensions: 1536\n",
    "# Each sentence has unique 1536-dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity: Measuring Semantic Distance\n",
    "\n",
    "To find similar meanings, we calculate **cosine similarity** between vectors:\n",
    "- **1.0** = identical meaning\n",
    "- **0.0** = unrelated\n",
    "- **-1.0** = opposite meaning\n",
    "\n",
    "Formula: `similarity = (A ¬∑ B) / (||A|| √ó ||B||)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Compare sentence 0 with others\n",
    "print(\"Similarity Scores:\")\n",
    "print(f\"Sentence 0 vs 1 (both about weather): {cosine_similarity(embeddings[0], embeddings[1]):.4f}\")\n",
    "print(f\"Sentence 0 vs 2 (weather vs programming): {cosine_similarity(embeddings[0], embeddings[2]):.4f}\")\n",
    "\n",
    "# Expected output:\n",
    "# Sentence 0 vs 1: ~0.85 (high similarity - both about weather)\n",
    "# Sentence 0 vs 2: ~0.12 (low similarity - different topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** The weather sentences have high similarity (~0.85) despite different words. The programming sentence has low similarity (~0.12). This is semantic search in action!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Setting Up\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "**Required:**\n",
    "1. Python 3.8+\n",
    "2. OpenAI API key (for embeddings)\n",
    "3. Pinecone API key (for vector database)\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "- `openai==1.46.0` - Embedding generation\n",
    "- `pinecone-client==5.0.1` - Vector database\n",
    "- `numpy==1.26.4` - Similarity calculations\n",
    "- `python-dotenv==1.0.1` - Environment config\n",
    "- `chromadb==0.5.5` - Alternative comparison (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Configuration\n",
    "\n",
    "**Step 1: Create `.env` file**\n",
    "```bash\n",
    "cp .env.example .env\n",
    "```\n",
    "\n",
    "**Step 2: Add your API keys**\n",
    "```env\n",
    "OPENAI_API_KEY=sk-proj-...\n",
    "PINECONE_API_KEY=pcsk_...\n",
    "PINECONE_REGION=us-east-1\n",
    "```\n",
    "\n",
    "**Get API keys:**\n",
    "- OpenAI: https://platform.openai.com/api-keys\n",
    "- Pinecone: https://app.pinecone.io/ (free tier available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify configuration\n",
    "import config\n",
    "\n",
    "print(\"Configuration Validation\")\n",
    "print(\"=\" * 50)\n",
    "config.validate_config()\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Expected output:\n",
    "# ‚úì OPENAI_API_KEY is set\n",
    "# ‚úì PINECONE_API_KEY is set\n",
    "# ‚úì PINECONE_REGION: us-east-1\n",
    "# ‚úì EMBEDDING_MODEL: text-embedding-3-small\n",
    "# ‚úì EMBEDDING_DIM: 1536\n",
    "# ‚úì INDEX_NAME: tvh-m1-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "\n",
    "openai_client, pinecone_client = config.get_clients()\n",
    "\n",
    "print(\"‚úì OpenAI client initialized\")\n",
    "print(\"‚úì Pinecone client initialized\")\n",
    "print(\"\\nReady to create vector database!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Understanding the Configuration\n\n**Key Constants (from `config.py`):**\n\n```python\nEMBEDDING_MODEL = \"text-embedding-3-small\"  # 1536 dimensions\nEMBEDDING_DIM = 1536                         # MUST match model!\nINDEX_NAME = \"tvh-m1-vectors\"               # Your index name\nDEFAULT_NAMESPACE = \"demo\"                   # Data partition\nSCORE_THRESHOLD = 0.7                        # Minimum similarity\nBATCH_SIZE = 100                             # Vectors per batch\n```\n\n**Critical:** `EMBEDDING_DIM` must match your model:\n- `text-embedding-3-small` ‚Üí 1536 dimensions\n- `text-embedding-3-large` ‚Üí 3072 dimensions\n\nMismatch = **dimension error** (we'll debug this in Section 6).\n\n---\n\n## Section 3: Pinecone Basics\n\n### Creating an Index\n\nA Pinecone **index** is like a database table for vectors. You must specify:\n- **Dimension**: Must match your embedding model (1536 for text-embedding-3-small)\n- **Metric**: Distance calculation method (cosine, euclidean, or dotproduct)\n- **Spec**: Deployment type (serverless or pod-based)\n\n**Important:** Indexes take 30-60 seconds to initialize. We'll wait for readiness."
  },
  {
   "cell_type": "code",
   "source": "# Create Pinecone index with readiness polling\nfrom m1_1_vector_databases import create_index_and_wait_pinecone\n\nindex = create_index_and_wait_pinecone(\n    pinecone_client,\n    index_name=config.INDEX_NAME,\n    dimension=config.EMBEDDING_DIM,\n    metric=config.METRIC\n)\n\nprint(f\"‚úì Index '{config.INDEX_NAME}' is ready!\")\nprint(f\"\\nIndex stats:\")\nprint(index.describe_index_stats())\n\n# Expected output:\n# Creating index: tvh-m1-vectors\n#   Dimension: 1536\n#   Metric: cosine\n#   Region: us-east-1\n# Waiting for index initialization...\n#   Index still initializing...\n# ‚úì Index ready after 42.3 seconds",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Namespaces: Organizing Your Data\n\n**Namespaces** partition data within a single index. Use cases:\n- Multi-tenancy (separate data per user/organization)\n- Environment isolation (dev/staging/production)\n- Domain separation (tech docs vs financial docs)\n\n**Benefits:**\n- Query isolation (User A can't see User B's data)\n- Cost efficiency (one index, multiple tenants)\n- Performance (filter before search)\n\nWe'll use the `demo` namespace for this tutorial.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 4: Upserting Data\n\n### Loading Example Data\n\nLet's load our example documents (diverse topics for testing semantic search):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load example texts\nfrom m1_1_vector_databases import load_example_texts\n\ntexts = load_example_texts(\"example_data.txt\")\n\nprint(f\"Loaded {len(texts)} documents\")\nprint(\"\\nFirst 3 documents:\")\nfor i, text in enumerate(texts[:3]):\n    print(f\"{i+1}. {text}\")\n\n# Expected output:\n# Loaded 20 documents\n# 1. Vector databases enable semantic search using embeddings...\n# 2. Pinecone is a managed vector database designed...\n# 3. Climate change is causing rising sea levels...",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Generating Embeddings with Rate Limit Handling\n\n**Important:** OpenAI has rate limits:\n- Free tier: ~3,000 requests/minute\n- Paid tier: ~10,000 requests/minute\n\nOur `embed_texts_openai` function includes **exponential backoff** to handle rate limits gracefully.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate embeddings with retry logic\nfrom m1_1_vector_databases import embed_texts_openai\nfrom datetime import datetime\n\nprint(f\"Generating embeddings at {datetime.now().strftime('%H:%M:%S')}...\")\n\nembeddings = embed_texts_openai(\n    texts,\n    client=openai_client,\n    model=config.EMBEDDING_MODEL,\n    max_retries=3\n)\n\nprint(f\"\\n‚úì Generated {len(embeddings)} embeddings\")\nprint(f\"  Dimension: {len(embeddings[0])}\")\nprint(f\"  Model: {config.EMBEDDING_MODEL}\")\n\n# Expected output:\n# Generating embeddings for 20 texts using text-embedding-3-small\n# Embedding texts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.67it/s]\n# ‚úì Generated 20 embeddings (dimension: 1536)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Upserting with Rich Metadata\n\n**Critical:** Always store metadata with your vectors!\n\n**Required metadata:**\n- `text`: The original document content (for retrieval)\n\n**Recommended metadata:**\n- `source`: Where the document came from\n- `chunk_id`: Position in original document\n- `timestamp`: When it was indexed\n- Custom fields for filtering (category, user_id, etc.)\n\n**Why metadata matters:**\n- Without `text`, you only get IDs and scores (useless!)\n- Filtering enables multi-tenancy and domain restriction\n- Debugging and auditing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prepare vectors with rich metadata\nfrom m1_1_vector_databases import upsert_vectors\n\nvectors = []\nfor i, (text, embedding) in enumerate(zip(texts, embeddings)):\n    vectors.append((\n        f\"doc_{i}\",  # Unique ID\n        embedding,   # 1536-D vector\n        {\n            \"text\": text,                            # Original content\n            \"source\": \"example_data.txt\",           # Source file\n            \"chunk_id\": i,                          # Position\n            \"timestamp\": datetime.utcnow().isoformat(),  # When indexed\n            \"length\": len(text)                     # Character count\n        }\n    ))\n\n# Upsert to Pinecone (batched automatically)\nstats = upsert_vectors(\n    index,\n    vectors,\n    namespace=config.DEFAULT_NAMESPACE,\n    batch_size=config.BATCH_SIZE\n)\n\nprint(\"\\n‚úì Upsert complete!\")\nprint(f\"  Upserted: {stats['upserted']} vectors\")\nprint(f\"  Namespace: {stats['namespace']}\")\nprint(f\"  Total in namespace: {stats['total_in_namespace']}\")\n\n# Expected output:\n# Upserting 20 vectors to namespace 'demo'\n# Batch size: 100\n#   Batch 1: Upserted 20/20 vectors\n# ‚úì Upsert complete: 20 vectors",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Cost & Latency Considerations\n\n**Embedding Costs (OpenAI):**\n- `text-embedding-3-small`: $0.02 per 1M tokens\n- ~750 words = 1,000 tokens\n- 20 documents (~3,000 words) ‚âà $0.00008 (negligible)\n\n**Pinecone Costs:**\n- Free tier: 100K vectors, 1 pod (adequate for learning)\n- Serverless: $70/month + $0.40 per 1M queries\n- Pod-based: $200+/month for dedicated capacity\n\n**Latency breakdown:**\n- Embedding generation: 10-50ms per request\n- Pinecone upsert: 30-80ms (batched)\n- Pinecone query: 30-80ms\n- **Total query pipeline: 50-150ms minimum**\n\n**Production tips:**\n- Batch embeddings (reduce API overhead)\n- Use namespaces strategically (reduce query scope)\n- Monitor usage in Pinecone console\n- Consider caching for frequently queried embeddings\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Querying & Filtering\n\n### Basic Semantic Search\n\nLet's query our vector database with natural language:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Query 1: Vector search basics\nfrom m1_1_vector_databases import query_pinecone\n\nquery1 = \"What is vector search?\"\n\nresults = query_pinecone(\n    index,\n    query1,\n    client=openai_client,\n    top_k=3,\n    namespace=config.DEFAULT_NAMESPACE,\n    score_threshold=0.7\n)\n\nprint(f\"Query: '{query1}'\\\\n\")\nfor i, result in enumerate(results, 1):\n    print(f\"{i}. Score: {result['score']:.4f}\")\n    print(f\"   Text: {result['text'][:100]}...\")\n    print()\n\n# Expected output:\n# Query: 'What is vector search?'\n#\n# 1. Score: 0.8923\n#    Text: Vector databases enable semantic search using embeddings...\n#\n# 2. Score: 0.8156\n#    Text: Pinecone is a managed vector database designed...",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Query 2: Different topic\nquery2 = \"climate and environmental issues\"\n\nresults2 = query_pinecone(\n    index,\n    query2,\n    client=openai_client,\n    top_k=3,\n    score_threshold=0.7\n)\n\nprint(f\"Query: '{query2}'\\\\n\")\nfor i, result in enumerate(results2, 1):\n    print(f\"{i}. Score: {result['score']:.4f}\")\n    print(f\"   Text: {result['text'][:80]}...\")\n    print()\n\n# Expected output shows climate-related documents with high scores",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding Score Thresholds\n\n**Similarity scores range from -1 to 1 (cosine similarity):**\n- **0.9+**: Nearly identical meaning\n- **0.7-0.9**: Highly similar (recommended threshold for production)\n- **0.5-0.7**: Moderately similar\n- **<0.5**: Weakly related or unrelated\n\n**Why thresholds matter:**\n- Including low-score results pollutes your RAG context\n- LLM performance degrades with irrelevant information\n- Balance precision (high threshold) vs recall (low threshold)\n\nLet's demonstrate the impact of threshold:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare thresholds\nquery = \"machine learning concepts\"\n\nprint(\"Threshold Comparison:\\\\n\")\n\nfor threshold in [0.5, 0.7, 0.9]:\n    results = query_pinecone(\n        index,\n        query,\n        client=openai_client,\n        top_k=5,\n        score_threshold=threshold\n    )\n    print(f\"Threshold {threshold}: {len(results)} results\")\n    if results:\n        print(f\"  Top score: {results[0]['score']:.4f}\")\n    print()\n\n# Expected output shows fewer results with higher thresholds\n# Threshold 0.5: 5 results (some may be irrelevant)\n# Threshold 0.7: 3 results (good quality)\n# Threshold 0.9: 1 result (very strict)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 6: Debugging & Common Failures\n\nThis section covers the 5 most common errors you'll encounter and how to fix them.\n\n### Failure #1: Dimension Mismatch\n\n**Symptom:**\n```\nPineconeException: Vector dimension 3072 does not match index dimension 1536\n```\n\n**Cause:** Using wrong embedding model for your index dimension.\n\n**Demo (will fail intentionally):**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# DON'T RUN THIS - it will error!\n# Demonstrating dimension mismatch\n\n# Wrong model (3072-D) for 1536-D index\n# response = client.embeddings.create(\n#     model=\"text-embedding-3-large\",  # 3072 dimensions!\n#     input=\"test\"\n# )\n# index.upsert([{\"id\": \"test\", \"values\": response.data[0].embedding}])\n# ‚Üí ERROR: Dimension mismatch!\n\nprint(\"‚úì Skipped dimension mismatch demo (would error)\")\nprint(\"\\\\nFix: Always match model dimension to index dimension\")\nprint(\"text-embedding-3-small ‚Üí 1536-D index\")\nprint(\"text-embedding-3-large ‚Üí 3072-D index\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Failure #2: Missing Metadata\n\n**Symptom:**\n```python\nKeyError: 'text'  # or empty metadata: {}\n```\n\n**Cause:** Stored vectors without metadata, can't retrieve original text.\n\n**Fix:** Always include `text` field in metadata:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Bad: No metadata\nbad_vector = {\n    \"id\": \"bad_example\",\n    \"values\": [0.1] * 1536  # Just the vector, no context!\n}\n\n# Good: Rich metadata\ngood_vector = {\n    \"id\": \"good_example\",\n    \"values\": [0.1] * 1536,\n    \"metadata\": {\n        \"text\": \"Always store the original text!\",  # REQUIRED\n        \"source\": \"best_practices.md\",\n        \"chunk_id\": 42,\n        \"category\": \"tutorial\"\n    }\n}\n\nprint(\"Bad metadata causes retrieval failures!\")\nprint(\"Good metadata enables:\")\nprint(\"  ‚úì LLM context generation\")\nprint(\"  ‚úì Source attribution\")\nprint(\"  ‚úì Metadata filtering\")\nprint(\"  ‚úì Debugging\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Remaining Common Failures\n\n**Failure #3: Ignoring Similarity Scores**\n- **Problem:** Including all results regardless of score\n- **Fix:** Apply threshold (0.7 recommended for production)\n- **Detection:** Monitor score distribution in logs\n\n**Failure #4: Rate Limit Exceeded**\n- **Problem:** Hitting OpenAI rate limits (429 error)\n- **Fix:** Implement exponential backoff (already in `embed_texts_openai`)\n- **Prevention:** Batch requests, add delays between batches\n\n**Failure #5: Index Not Ready**\n- **Problem:** Querying immediately after index creation\n- **Fix:** Use `create_index_and_wait_pinecone` (polls for readiness)\n- **Prevention:** Always wait 30-60s for index initialization\n\n**Debugging Checklist:**\n- [ ] Embedding dimension matches index dimension\n- [ ] Metadata includes 'text' field\n- [ ] Score threshold is appropriate (0.7 default)\n- [ ] Rate limiting is handled\n- [ ] Index is ready before upserting/querying\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Bonus: ChromaDB Quick Comparison\n\nChromaDB is an **in-process** vector database - great for prototyping and small datasets.\n\n**When to use ChromaDB:**\n- Prototyping and local development\n- Datasets < 1M vectors\n- Cost-sensitive projects (free, open-source)\n- Complete data control required\n\n**When to use Pinecone:**\n- Production workloads > 1M vectors\n- Need managed infrastructure\n- Multi-tenancy requirements\n- Guaranteed 99.9% uptime SLA\n\n**Quick ChromaDB demo:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ChromaDB demo (optional - requires chromadb package)\nimport chromadb\n\n# Create in-memory client\nchroma_client = chromadb.Client()\n\n# Create collection\ncollection = chroma_client.create_collection(name=\"demo_collection\")\n\n# Add documents (ChromaDB handles embeddings automatically!)\ncollection.add(\n    documents=[\n        \"Vector databases enable semantic search\",\n        \"ChromaDB is an open-source alternative\",\n        \"Pinecone offers managed infrastructure\"\n    ],\n    ids=[\"doc1\", \"doc2\", \"doc3\"]\n)\n\n# Query\nchroma_results = collection.query(\n    query_texts=[\"What are vector database options?\"],\n    n_results=2\n)\n\nprint(\"ChromaDB Results:\")\nfor doc, distance in zip(chroma_results['documents'][0], chroma_results['distances'][0]):\n    print(f\"  - {doc} (distance: {distance:.4f})\")\n\nprint(\"\\\\n‚úì ChromaDB: Simple for prototyping, but lacks production features\")\nprint(\"‚úì Pinecone: Production-ready, managed, scalable\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Summary & Next Steps\n\n### What You Learned\n\n‚úì **Vector embeddings** convert text to 1536-D numerical representations  \n‚úì **Cosine similarity** measures semantic distance (-1 to 1)  \n‚úì **Pinecone indexes** store millions of vectors with ANN search  \n‚úì **Namespaces** enable multi-tenancy and data isolation  \n‚úì **Metadata** is critical for retrieval and filtering  \n‚úì **Score thresholds** (0.7 recommended) filter low-quality matches  \n‚úì **5 common failures** and how to debug them\n\n### Decision Card: When to Use Vector Databases\n\n**‚úÖ Use when:**\n- Semantic search is primary requirement\n- Dataset > 10K documents\n- Can accept 50-100ms latency overhead\n- Need managed infrastructure\n\n**‚ùå Avoid when:**\n- Exact keyword matching required (legal/compliance)\n- Dataset < 1K documents\n- Real-time freshness < 1 second\n- Budget constraints (use ChromaDB/pgvector)\n\n### Production Checklist\n\nBefore deploying to production:\n- [ ] Match embedding dimension to index dimension\n- [ ] Store comprehensive metadata (especially 'text')\n- [ ] Implement retry logic for rate limits\n- [ ] Set appropriate score threshold (test on your data)\n- [ ] Monitor costs in Pinecone console\n- [ ] Use namespaces for multi-tenancy\n- [ ] Batch operations (100-200 vectors)\n- [ ] Wait for index readiness before operations\n\n### Next Steps\n\n1. **Practice:** Complete the CLI tool exercises in README.md\n2. **Experiment:** Try different embedding models and thresholds\n3. **Compare:** Test ChromaDB vs Pinecone on your use case\n4. **Build:** Create a simple Q&A system with your own documents\n5. **Monitor:** Track costs and latency in production\n\n**Resources:**\n- Pinecone docs: https://docs.pinecone.io/\n- OpenAI embeddings: https://platform.openai.com/docs/guides/embeddings\n- ChromaDB: https://docs.trychroma.com/\n- Course repo: Check README.md for challenges\n\n---\n\n**Congratulations!** You now understand vector databases and are ready to build production RAG systems. üéâ",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}