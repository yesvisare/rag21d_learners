{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.2 — Pinecone Data Model & Advanced Indexing\n",
    "\n",
    "**Hybrid Search, Namespaces, Failures, Decision Framework**\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Dense (semantic) + Sparse (keyword) hybrid search\n",
    "- Alpha parameter tuning for query-specific blending\n",
    "- Namespace-based multi-tenant isolation\n",
    "- Production failure scenarios and fixes\n",
    "- Decision framework for when to use hybrid search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pinecone Data Model (Index, Namespace, Vector)\n",
    "\n",
    "Pinecone organizes data in a three-level hierarchy:\n",
    "\n",
    "**Index** → Database container with fixed dimension and metric  \n",
    "**Namespace** → Isolated partition within index for multi-tenancy  \n",
    "**Vector** → Individual record with:\n",
    "- `id`: Unique identifier (up to 512 chars)\n",
    "- `values`: Dense embedding (e.g., 1536-dim from OpenAI)\n",
    "- `sparse_values`: Optional BM25 keyword vector (indices + values)\n",
    "- `metadata`: Key-value pairs (max 40KB per vector)\n",
    "\n",
    "Hybrid vectors combine both dense (semantic similarity) and sparse (exact keyword matching) representations in a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example vector structure\n",
    "example_vector = {\n",
    "    \"id\": \"doc_0\",\n",
    "    \"values\": [0.023, -0.154, 0.091],  # Dense embedding (showing first 3 of 1536)\n",
    "    \"sparse_values\": {\n",
    "        \"indices\": [42, 137, 891],  # BM25 term indices\n",
    "        \"values\": [0.85, 0.62, 0.41]  # BM25 term weights\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"text\": \"Machine learning models require...\",\n",
    "        \"source\": \"example_data\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Pinecone Hybrid Vector Structure:\")\n",
    "print(f\"  ID: {example_vector['id']}\")\n",
    "print(f\"  Dense dim: 1536 (showing first 3: {example_vector['values']})\")\n",
    "print(f\"  Sparse terms: {len(example_vector['sparse_values']['indices'])}\")\n",
    "print(f\"  Metadata keys: {list(example_vector['metadata'].keys())}\")\n",
    "\n",
    "# Expected: 4 lines showing vector structure components"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Dense vs Sparse Recap (Semantic vs Keyword)\n\n**Dense Embeddings (Semantic):**\n- Captures meaning and context through learned representations\n- 1536 dimensions from OpenAI's text-embedding-3-small\n- Excellent for natural language questions, conceptual searches\n- Query latency: 40-60ms\n- Misses exact keyword matches (e.g., product codes, IDs)\n\n**Sparse Embeddings (Keyword):**\n- BM25 algorithm: term frequency × inverse document frequency\n- Only non-zero values stored (efficient for vocabulary size)\n- Perfect for exact matches, product names, technical codes\n- Query latency: <20ms\n- Completely misses semantic similarity\n\n**Why Hybrid?**\nCombining both approaches improves recall by 20-40% for mixed-intent queries. However, it adds 30-80ms latency and requires alpha tuning per use case.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from m1_2_pinecone_advanced_indexing import embed_dense_openai, embed_sparse_bm25\nfrom config import get_clients\n\n# Check if API keys are available\nopenai_client, _ = get_clients()\n\nif openai_client:\n    # Show real dense embedding dimension\n    sample_text = \"Machine learning models require careful tuning\"\n    dense_vec = embed_dense_openai(sample_text)\n    print(f\"Dense embedding: {len(dense_vec)} dimensions\")\n    print(f\"Sample values: {dense_vec[:3]}\")\nelse:\n    print(\"Dense embedding: 1536 dimensions (stub, no API key)\")\n    print(\"Sample values: [0.023, -0.154, 0.091]\")\n\n# Show sparse embedding structure\ncorpus = [\"machine learning\", \"deep learning\", \"neural networks\"]\nembed_sparse_bm25(texts=corpus)  # Fit BM25\nsparse_vec = embed_sparse_bm25(query=\"machine learning models\")\nprint(f\"\\nSparse embedding: {len(sparse_vec['indices'])} non-zero terms\")\nprint(f\"Sample indices: {sparse_vec['indices'][:3]}\")\nprint(f\"Sample values: {[round(v, 3) for v in sparse_vec['values'][:3]]}\")\n\n# Expected: 5-6 lines showing dense dimension and sparse term counts",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Building a Hybrid Index (dotproduct, 1536)\n\n**Index Configuration Requirements:**\n- **Metric**: Must use `dotproduct` (not cosine or euclidean)  \n  Why? Hybrid search scales vectors by alpha weights, requiring linear combination\n- **Dimension**: 1536 for OpenAI text-embedding-3-small\n- **Cloud**: Serverless deployment (auto-scaling, pay-per-use)\n\n**Critical Configuration Check:**\nMismatched metrics cause silent failures where alpha blending produces incorrect scores. Always validate `metric == \"dotproduct\"` before querying.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from m1_2_pinecone_advanced_indexing import build_index\nfrom config import get_clients\n\n# Attempt to build/connect to hybrid index\nprint(\"Building Hybrid Index...\")\nprint(\"=\" * 50)\n\nindex = build_index(dimension=1536, metric=\"dotproduct\")\n\nif index is None:\n    # No API keys - show configuration that would be used\n    print(\"Configuration (not applied, no API keys):\")\n    print(\"  Name: hybrid-rag\")\n    print(\"  Dimension: 1536\")\n    print(\"  Metric: dotproduct\")\n    print(\"  Cloud: AWS us-east-1 serverless\")\nelse:\n    print(f\"✓ Index ready: {index}\")\n    print(f\"Stats: {index.describe_index_stats()}\")\n\n# Expected: 3-5 lines showing index config or connection status",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Querying with Alpha (0.2, 0.5, 0.8)\n\n**Alpha Parameter Controls Blending:**\n- `α = 0.0` → Pure sparse (100% keyword/BM25)\n- `α = 0.5` → Balanced hybrid (50/50 split)\n- `α = 1.0` → Pure dense (100% semantic)\n\n**Selection Strategy:**\n- **α = 0.2-0.3**: Keyword-heavy queries (product codes, IDs, exact terms)\n- **α = 0.5**: Balanced/uncertain (default starting point)\n- **α = 0.7-0.8**: Semantic-heavy queries (natural language questions, concepts)\n\n**How It Works:**\n```python\ndense_scaled = dense_vector * alpha\nsparse_scaled = sparse_vector * (1 - alpha)\ncombined_score = dotproduct(dense_scaled) + dotproduct(sparse_scaled)\n```\n\nExperimentation is required — initial tuning typically takes 4-8 hours per use case.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from m1_2_pinecone_advanced_indexing import hybrid_query\nfrom config import get_clients\n\n# Test query with different alpha values\ntest_query = \"explain machine learning hyperparameter tuning\"\n\nprint(f\"Query: '{test_query}'\")\nprint(\"=\" * 60)\n\n# Check if we can run real queries\nopenai_client, pinecone_client = get_clients()\n\nif openai_client and pinecone_client:\n    # Real queries (requires data to be upserted first)\n    for alpha in [0.2, 0.5, 0.8]:\n        print(f\"\\nAlpha = {alpha} ({'keyword-heavy' if alpha < 0.4 else 'balanced' if alpha < 0.6 else 'semantic-heavy'})\")\n        results = hybrid_query(test_query, alpha=alpha, top_k=3)\n        for i, res in enumerate(results[:3], 1):\n            print(f\"  {i}. [{res['score']:.4f}] {res['text'][:80]}...\")\nelse:\n    # Simulated results (no API keys)\n    print(\"⚠️ Simulating results (no API keys)\\n\")\n    for alpha in [0.2, 0.5, 0.8]:\n        print(f\"Alpha = {alpha}:\")\n        print(f\"  1. [0.8234] Machine learning models require careful hyperparameter tuning for opt...\")\n        print(f\"  2. [0.7891] Kubernetes orchestrates containerized applications across distributed...\")\n        print(f\"  3. [0.7456] Neural architecture search automates the design of deep learning mode...\")\n        print()\n\n# Expected: Top 3 results per alpha (9 result lines total, or simulated equivalents)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Namespaces & Multi-Tenant (user-isolated search)\n\n**What Are Namespaces?**\nNamespaces are isolated partitions within a single Pinecone index. They enable:\n- **Multi-tenancy**: Each customer/user gets their own namespace\n- **Data isolation**: Queries only search within the specified namespace\n- **Cost efficiency**: Share infrastructure without separate indexes\n- **Access control**: Validate namespace existence before queries\n\n**Use Cases:**\n- SaaS applications with per-customer data\n- Team-based document repositories\n- A/B testing with variant-specific namespaces\n- Environment separation (dev/staging/prod)\n\n**Important Notes:**\n- Namespaces share the same BM25 vocabulary (potential cross-contamination)\n- For strict isolation or different schemas, use separate indexes\n- Always validate namespace exists before querying (prevents empty results)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from m1_2_pinecone_advanced_indexing import upsert_hybrid_vectors, hybrid_query\nfrom config import get_clients\n\n# Create namespace-specific data\nuser_namespace = \"user-123\"\nuser_docs = [\n    \"User 123 compliance report Q4 2024\",\n    \"Risk assessment for user account 123\",\n    \"Transaction history for customer ID 123\"\n]\n\nprint(f\"Multi-Tenant Namespace Demo: '{user_namespace}'\")\nprint(\"=\" * 60)\n\nopenai_client, pinecone_client = get_clients()\n\nif openai_client and pinecone_client:\n    # Upsert to user-specific namespace\n    print(f\"\\nUpserting {len(user_docs)} docs to namespace '{user_namespace}'...\")\n    result = upsert_hybrid_vectors(user_docs, namespace=user_namespace)\n    print(f\"✓ Upsert result: {result}\")\n    \n    # Query only within this namespace\n    print(f\"\\nQuerying namespace '{user_namespace}'...\")\n    results = hybrid_query(\"compliance report\", alpha=0.5, top_k=2, namespace=user_namespace)\n    for i, res in enumerate(results, 1):\n        print(f\"  {i}. [{res['score']:.4f}] {res['text'][:60]}...\")\nelse:\n    # Show payload shape when no keys\n    print(\"\\n⚠️ Simulating upsert (no API keys)\")\n    print(f\"Namespace: '{user_namespace}'\")\n    print(f\"Docs: {len(user_docs)}\")\n    print(\"Payload shape:\")\n    print(\"  {id: 'doc_0', values: [1536-dim], sparse_values: {...}, metadata: {...}}\")\n    print(f\"\\nQuery would target namespace: '{user_namespace}' only\")\n\n# Expected: 3-5 lines showing namespace upsert/query or simulated structure",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Common Failures & Fixes (5 scenarios)\n\nProduction hybrid search has predictable failure modes. All are handled in `m1_2_pinecone_advanced_indexing.py`.\n\n### Failure #1: BM25 Not Fitted\n**Symptom**: `ValueError` when calling `embed_sparse_bm25(query=...)`  \n**Cause**: Forgot to fit BM25 on corpus before encoding queries  \n**Fix**: Always call `embed_sparse_bm25(texts=corpus)` first  \n**Detection**: Check `_bm25_encoder is not None` before queries\n\n### Failure #2: Metric Mismatch\n**Symptom**: Incorrect scores, alpha blending doesn't work as expected  \n**Cause**: Index created with `cosine` or `euclidean` instead of `dotproduct`  \n**Fix**: Recreate index with `metric=\"dotproduct\"` (required for hybrid)  \n**Detection**: Validate `index.metric == \"dotproduct\"` on startup\n\n### Failure #3: Missing Namespace\n**Symptom**: Empty results when querying valid data  \n**Cause**: Querying namespace that doesn't exist (typo, not yet created)  \n**Fix**: Use `safe_namespace_query()` to validate before querying  \n**Detection**: Check `namespace in index.describe_index_stats()[\"namespaces\"]`\n\n### Failure #4: Metadata Size Exceeds 40KB\n**Symptom**: Upsert fails with metadata size error  \n**Cause**: Long text fields or too many metadata keys  \n**Fix**: Truncate text, remove unnecessary fields, store full text externally  \n**Detection**: Use `validate_metadata_size()` before upsert\n\n### Failure #5: Partial Batch Failures\n**Symptom**: Some vectors fail to upsert (API errors, dimension mismatch)  \n**Cause**: Network issues, rate limits, or data validation failures  \n**Fix**: Track failed IDs, implement retry logic with exponential backoff  \n**Detection**: Check upsert response for `failed_ids`, log failures",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from m1_2_pinecone_advanced_indexing import (\n    check_bm25_fitted, validate_metadata_size, safe_namespace_query\n)\nfrom pinecone_text.sparse import BM25Encoder\nimport sys\n\nprint(\"Testing 5 Common Failure Scenarios\")\nprint(\"=\" * 60)\n\n# #1: BM25 Not Fitted\nprint(\"\\n#1 BM25 Fitted Check:\")\nis_fitted = check_bm25_fitted()\nprint(f\"  ✓ BM25 encoder fitted: {is_fitted}\")\n\n# #2: Metric Check (simulated - requires real index)\nprint(\"\\n#2 Metric Validation:\")\nprint(\"  ✓ Index must use metric='dotproduct' for hybrid search\")\nprint(\"  Check: index.metric == 'dotproduct' before queries\")\n\n# #3: Namespace Validation (simulated)\nprint(\"\\n#3 Namespace Existence Check:\")\nprint(\"  ✓ Use safe_namespace_query() to validate before querying\")\nprint(\"  Prevents: Empty results from typos/missing namespaces\")\n\n# #4: Metadata Size Validation\nprint(\"\\n#4 Metadata Size Validation:\")\ntiny_meta = {\"text\": \"short\", \"id\": \"123\"}\nlarge_meta = {\"text\": \"x\" * 50000, \"more\": \"data\"}  # Exceeds 40KB\ntry:\n    validate_metadata_size(tiny_meta)\n    print(\"  ✓ Small metadata passed (<40KB)\")\nexcept ValueError as e:\n    print(f\"  ❌ {e}\")\n\ntry:\n    validate_metadata_size(large_meta)\nexcept ValueError as e:\n    print(f\"  ✓ Large metadata caught: {str(e)[:60]}...\")\n\n# #5: Batch Failure Handling\nprint(\"\\n#5 Partial Batch Failures:\")\nprint(\"  ✓ upsert_hybrid_vectors() returns failed_ids list\")\nprint(\"  Enables: Retry logic, error logging, graceful degradation\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"All failure modes have defensive checks in place\")\n\n# Expected: 5 blocks showing each failure check result (~15 lines total)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Decision Card & Production Notes\n\n### When to Use Hybrid Search\n\n**✅ Use Hybrid When:**\n- Mixed-intent queries (semantic + exact keywords)\n- Need 20-40% better recall than dense-only\n- Acceptable to invest 4-8 hours alpha tuning per use case\n- Queries blend natural language with product codes/IDs\n- Willing to accept 30-80ms additional latency\n\n**❌ Avoid Hybrid When:**\n- 70%+ queries are purely keyword-based → Use Elasticsearch/traditional search\n- 70%+ queries are semantic-only → Use dense vectors (M1.1)\n- Sub-50ms latency required → Dense-only is faster (40-60ms vs 60-120ms)\n- Corpus updates hourly → BM25 refitting overhead too high (5-15 min per 10K docs)\n- No time for alpha experimentation → Stick with proven dense search\n\n---\n\n### Benefits\n- **Improved Recall**: 20-40% better than dense-only for mixed queries\n- **Keyword Precision**: Catches exact matches (product codes, IDs, legal terms)\n- **Namespace Isolation**: Multi-tenancy without separate indexes\n- **Production Ready**: Built-in failure handling for all 5 common scenarios\n\n### Limitations\n- **Latency Overhead**: +30-80ms per query vs dense-only\n- **Alpha Tuning**: 4-8 hours per use case to find optimal blend\n- **BM25 Refitting**: 5-15 min per 10K docs on corpus updates\n- **Metric Lock-In**: Must use `dotproduct` (can't use cosine/euclidean)\n- **Namespace Vocabulary**: BM25 shared across namespaces (potential contamination)\n\n---\n\n### Cost Estimates (Monthly)\n\n| Scale | Queries/Day | Vectors | Embedding Cost | Pinecone Cost | Total |\n|-------|-------------|---------|----------------|---------------|-------|\n| Dev/Test | 100 | 1K | $5 | $15 | **$20** |\n| Small | 1K | 10K | $25 | $75 | **$100** |\n| Medium | 10K | 100K | $250 | $450 | **$700** |\n| Large | 100K | 1M | $2,500 | $3,200 | **$5,700** |\n\n*Assumptions: text-embedding-3-small ($0.020/1M tokens), Pinecone serverless (us-east-1), 2 queries per upsert*\n\n**Cost Drivers:**\n- OpenAI embeddings: Scales with text length and query volume\n- Pinecone storage: $0.40/1M vectors/month (serverless)\n- Pinecone queries: Varies by p1 pods vs serverless\n\n---\n\n### Monitoring & Production Checklist\n\n**Key Metrics:**\n- Query latency (p50, p95, p99) — target <120ms hybrid, <60ms dense-only\n- BM25 refit duration — track for corpus update planning\n- Namespace query distribution — detect skew/hotspots\n- Failed upsert rate — should be <0.1%\n- Alpha effectiveness per query type — log for continuous tuning\n\n**Operational Notes:**\n- Schedule BM25 refitting during low-traffic windows\n- Implement query-level alpha selection (smart_alpha_selector)\n- Cache frequently accessed queries (Redis/Memcached)\n- Monitor Pinecone quota limits (free tier: 100K operations/month)\n- Implement exponential backoff for rate limit errors\n\n---\n\n### Next Steps\n\n**Completed in M1.2:**\n- ✅ Hybrid search implementation (dense + sparse)\n- ✅ Alpha parameter tuning strategy\n- ✅ Namespace-based multi-tenancy\n- ✅ Production failure handling (5 scenarios)\n- ✅ Decision framework (use when/avoid when)\n\n**Next Module: M1.3 — Document Pipeline & Chunking**\n- Document loaders (PDF, DOCX, HTML)\n- Chunking strategies (fixed, semantic, recursive)\n- Metadata extraction & enrichment\n- End-to-end ingestion pipeline",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}