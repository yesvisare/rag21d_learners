{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1.4 ‚Äî Query Pipeline & Response Generation\n",
    "\n",
    "**Complete 7-Stage RAG Pipeline:**  \n",
    "Query ‚Üí Retrieval ‚Üí Rerank ‚Üí Context ‚Üí LLM ‚Üí Answer\n",
    "\n",
    "This notebook demonstrates the full production query pipeline with hybrid search, reranking, and intelligent context preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reality Check: What Query Pipelines Do / Don't Do\n",
    "\n",
    "**What RAG Query Pipelines DO:**\n",
    "- ‚úÖ Reduce hallucination by 60-80% through grounded responses\n",
    "- ‚úÖ Handle 1,000+ documents efficiently with proper indexing\n",
    "- ‚úÖ Provide source attribution for trust and verification\n",
    "- ‚úÖ Adapt to diverse query types (factual, how-to, troubleshooting, etc.)\n",
    "\n",
    "**What They DON'T Do:**\n",
    "- ‚ùå Eliminate all hallucinations (LLMs can still extrapolate)\n",
    "- ‚ùå Work without quality source data (garbage in, garbage out)\n",
    "- ‚ùå Handle multi-turn context automatically (requires session management)\n",
    "- ‚ùå Operate instantly (adds 200-400ms latency)\n",
    "\n",
    "**Key Trade-offs:**\n",
    "1. **Accuracy vs Latency:** Reranking improves quality but adds 50-100ms\n",
    "2. **Coverage vs Precision:** More chunks = better coverage but noisier context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reality Check Demonstration\n",
    "print(\"‚úÖ RAG Query Pipelines CAN:\")\n",
    "print(\"  ‚Ä¢ Reduce hallucination 60-80%\")\n",
    "print(\"  ‚Ä¢ Handle 1,000+ documents\")\n",
    "print(\"  ‚Ä¢ Provide source attribution\")\n",
    "print(\"  ‚Ä¢ Adapt to query types\\n\")\n",
    "\n",
    "print(\"‚ùå RAG Query Pipelines CANNOT:\")\n",
    "print(\"  ‚Ä¢ Eliminate all hallucinations\")\n",
    "print(\"  ‚Ä¢ Work with poor source data\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Key Trade-offs:\")\n",
    "print(\"  1. Accuracy vs Latency (reranking adds 50-100ms)\")\n",
    "print(\"  2. Coverage vs Precision (more chunks = more noise)\")\n",
    "\n",
    "# Expected: 4-line capabilities list + 2-line limitations + 2 trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Query Understanding (Type, Expansion, Keywords)\n\n**Stage 1 of 7:** Understanding user intent before retrieval.\n\n**Three Key Steps:**\n1. **Classification:** Identify query type (factual, how-to, comparison, definition, troubleshooting, opinion)\n2. **Expansion:** Generate alternative phrasings to capture more relevant documents (optional, LLM-based)\n3. **Keyword Extraction:** Extract key terms for filtering and metadata matching\n\n**Why It Matters:**\n- Different query types need different retrieval strategies (alpha tuning)\n- Query expansion can improve recall by 15-25%\n- Keywords enable metadata filtering for faster retrieval",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Query Understanding Demo\nfrom m1_4_query_pipeline import QueryProcessor, QueryType\nfrom config import get_clients\n\n# Initialize processor\nopenai_client, _ = get_clients()\nprocessor = QueryProcessor(openai_client)\n\n# Sample query\nsample_query = \"How do I improve RAG accuracy?\"\n\n# 1. Classify query type\nquery_type = processor.classify(sample_query)\nprint(f\"üìã Query Type: {query_type.value}\")\n\n# 2. Generate expansions (if keys exist)\nif openai_client:\n    expansions = processor.expand(sample_query, num_expansions=2)\n    print(f\"üîÑ Expansions ({len(expansions)}):\")\n    for i, exp in enumerate(expansions[:3], 1):\n        print(f\"  {i}. {exp[:70]}...\")\nelse:\n    print(\"‚ö†Ô∏è Skipping expansions (no API keys)\")\n\n# 3. Extract keywords\nkeywords = processor.extract_keywords(sample_query)\nprint(f\"üîë Keywords: {keywords[:6]}\")\n\n# Expected: Query type, 2-3 expansion examples, ‚â§6 keywords",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Retrieval Strategies (Hybrid Dense+Sparse, Alpha)\n\n**Stage 2 of 7:** Fetching relevant documents using hybrid search.\n\n**Hybrid Search Components:**\n1. **Dense Embeddings:** Semantic similarity via OpenAI text-embedding-3-small\n2. **Sparse Embeddings:** Keyword matching via BM25 encoding\n3. **Alpha Tuning:** Query-type specific weighting (0.3-0.8)\n\n**Alpha Values by Query Type:**\n- Factual/Definition: 0.7 (favor semantic understanding)\n- How-to: 0.5 (balanced)\n- Troubleshooting: 0.3 (favor exact terms/error codes)\n- Comparison/Opinion: 0.6 (moderate semantic bias)\n\n**Why Hybrid Works:**\n- Dense captures meaning: \"car\" matches \"vehicle\", \"automobile\"\n- Sparse captures exactness: \"error 404\" requires literal match\n- Alpha balancing optimizes for query intent",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Retrieval Strategy Demo\nfrom m1_4_query_pipeline import SmartRetriever, QueryType\n\n# Initialize retriever\nopenai_client, pinecone_client = get_clients()\nretriever = SmartRetriever(openai_client, pinecone_client)\n\n# Sample query\nsample_query = \"How do I improve RAG accuracy?\"\nquery_type = QueryType.HOW_TO\n\n# Get alpha for this query type\nalpha = retriever._get_alpha_for_query_type(query_type)\nprint(f\"‚öôÔ∏è Alpha for {query_type.value}: {alpha}\")\n\n# Retrieve results\nresults = retriever.retrieve(\n    query=sample_query,\n    query_type=query_type,\n    top_k=3,\n    namespace=\"demo\"\n)\n\nprint(f\"\\\\nüìÑ Retrieved {len(results)} results:\")\nfor i, result in enumerate(results[:3], 1):\n    preview = result.text[:80].replace('\\\\n', ' ')\n    print(f\"  {i}. [score={result.score:.3f}] {preview}...\")\n\n# Expected: Alpha value, top 3 results with score + 80-char preview",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Reranking with Cross-Encoder\n\n**Stage 3 of 7:** Refining initial results with deeper semantic scoring.\n\n**Why Rerank?**\n- Initial retrieval uses **bi-encoders** (fast, independent embeddings)\n- Reranking uses **cross-encoders** (slower, joint query-document encoding)\n- Cross-encoders achieve 10-20% better relevance at cost of 50-100ms latency\n\n**Model:** `cross-encoder/ms-marco-MiniLM-L-6-v2`\n- Trained on MS MARCO passage ranking\n- Optimized for query-document relevance scoring\n- Compact (6 layers) for production speed\n\n**Process:**\n1. Take top-K initial results (e.g., K=5)\n2. Score each with cross-encoder\n3. Re-sort by rerank_score (preserve original_score)\n4. Return top-N (e.g., N=3)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reranking Demo\nfrom m1_4_query_pipeline import Reranker\n\n# Initialize reranker\nreranker = Reranker()\n\n# Use results from previous retrieval\nprint(f\"üîÑ Reranking {len(results)} initial results...\")\n\n# Rerank\nreranked = reranker.rerank(\n    query=sample_query,\n    results=results,\n    top_k=3\n)\n\nprint(f\"\\\\nüèÜ Top 3 after reranking:\")\nfor i, result in enumerate(reranked[:3], 1):\n    orig_score = result.original_score or 0\n    rerank_score = result.rerank_score or 0\n    diff = rerank_score - orig_score\n    preview = result.text[:60].replace('\\\\n', ' ')\n    print(f\"  {i}. [rerank={rerank_score:.3f}, orig={orig_score:.3f}, Œî={diff:+.3f}]\")\n    print(f\"     {preview}...\")\n\n# Expected: Top 3 with rerank_score, original_score, and difference",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Context Preparation (Dedup, Sources, Limits)\n\n**Stage 4 of 7:** Building clean, attributed context for the LLM.\n\n**Three Critical Steps:**\n1. **Deduplication:** Remove redundant chunks (same content from multiple sources)\n2. **Source Attribution:** Tag each chunk with [Source N: source_name] for transparency\n3. **Length Guard:** Enforce max context length (e.g., 4000 chars) to fit token budget\n\n**Why It Matters:**\n- **Dedup:** Prevents wasting tokens on repeated information\n- **Sources:** Builds user trust; enables citation verification\n- **Length limits:** Ensures prompts fit within model context window\n\n**Token Budget Calculation:**\n- Model window: 8K tokens (GPT-4o-mini)\n- Reserve for response: 1K tokens\n- System prompt: ~200 tokens\n- Available for context: ~6.8K tokens (‚âà4000 characters)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Context Preparation Demo\nfrom m1_4_query_pipeline import ContextBuilder\n\n# Initialize builder\nbuilder = ContextBuilder(max_length=4000)\n\n# Build context with scores\ncontext_data = builder.context_with_scores(reranked)\n\nprint(f\"üì¶ Context Statistics:\")\nprint(f\"  ‚Ä¢ Chunks: {context_data['num_chunks']}\")\nprint(f\"  ‚Ä¢ Avg Score: {context_data['avg_score']:.3f}\")\nprint(f\"  ‚Ä¢ Unique Sources: {context_data['unique_sources']}\")\nprint(f\"  ‚Ä¢ Context Length: {len(context_data['context'])} chars\")\n\nprint(f\"\\\\nüìö Sources:\")\nfor i, source in enumerate(context_data['sources'][:3], 1):\n    print(f\"  {i}. {source}\")\n\n# Expected: num_chunks, avg_score, unique_sources count, sources list",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Prompt Engineering per Query Type\n\n**Stage 5 of 7:** Crafting query-type specific prompts for optimal responses.\n\n**Six Template Types:**\n1. **Factual:** \"Provide factual answers based strictly on context\"\n2. **How-to:** \"Provide clear, step-by-step instructions\"\n3. **Comparison:** \"Compare and contrast options\"\n4. **Definition:** \"Provide clear definitions\"\n5. **Troubleshooting:** \"Help diagnose and solve problems\"\n6. **Opinion:** \"Provide balanced perspectives\"\n\n**Template Structure:**\n- **System prompt:** Sets behavior and constraints\n- **User prompt:** Injects context + query with task-specific framing\n\n**Example (How-to):**\n- System: \"You are a helpful AI assistant. Provide clear, step-by-step instructions based on the given context.\"\n- User: \"Context: [retrieved chunks]\\n\\nQuestion: [query]\\n\\nProvide step-by-step instructions based on the context above.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prompt Engineering Demo\nfrom m1_4_query_pipeline import PromptBuilder\n\n# Initialize builder\nprompt_builder = PromptBuilder()\n\n# Build prompt for HOW_TO query type\nprompt = prompt_builder.build_prompt(\n    query=sample_query,\n    context=context_data['context'],\n    query_type=QueryType.HOW_TO\n)\n\nprint(f\"üéØ Query Type: {QueryType.HOW_TO.value}\")\nprint(f\"\\\\nüìù System Prompt (first 100 chars):\")\nprint(f\"  {prompt['system'][:100]}...\")\n\nprint(f\"\\\\nüìù User Prompt (first 200 chars):\")\nprint(f\"  {prompt['user'][:200]}...\")\n\n# Expected: System prompt preview + User prompt preview (‚â§200 chars each)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Response Generation (Non-Streaming & Streaming)\n\n**Stage 6 of 7:** Generating the final answer using the LLM.\n\n**Two Generation Modes:**\n\n**1. Non-Streaming (Batch):**\n- Wait for complete response before returning\n- Simpler error handling\n- Good for analytics, logging, caching\n- User waits 1-3 seconds for full answer\n\n**2. Streaming:**\n- Token-by-token delivery as generated\n- Better perceived latency (first token in ~300ms)\n- Enables real-time UI updates\n- Requires stream-aware error handling\n\n**Configuration:**\n- Model: `gpt-4o-mini` (fast, cost-effective)\n- Temperature: 0.1 (low for factual consistency)\n- Max tokens: 500 (constrains response length)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Response Generation Demo\nfrom m1_4_query_pipeline import ResponseGenerator\n\n# Initialize generator\ngenerator = ResponseGenerator(openai_client)\n\n# 1. Non-streaming generation\nprint(\"ü§ñ Non-Streaming Generation:\")\nif openai_client:\n    answer = generator.generate(prompt, temperature=0.1, max_tokens=200)\n    print(f\"  {answer[:150]}...\" if len(answer) > 150 else f\"  {answer}\")\nelse:\n    print(\"  ‚ö†Ô∏è Skipping API calls (no keys found)\")\n\n# 2. Streaming generation (first few tokens)\nprint(\"\\\\nüåä Streaming Generation (sample):\")\nif openai_client:\n    print(\"  \", end=\"\", flush=True)\n    token_count = 0\n    for chunk in generator.stream(prompt, temperature=0.1, max_tokens=200):\n        print(chunk, end=\"\", flush=True)\n        token_count += 1\n        if token_count >= 20:  # Limit to ~20 tokens for demo\n            print(\"...\")\n            break\n    print()\nelse:\n    print(\"  ‚ö†Ô∏è Skipping streaming (no keys found)\")\n\n# Expected: Non-stream answer (~150 chars) + streaming first ~20 tokens",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Complete Pipeline Run + Timings & Metadata\n\n**Stage 7 of 7:** End-to-end integration with metrics tracking.\n\n**Full Pipeline Flow:**\n1. Query Understanding ‚Üí Classification, expansion, keywords\n2. Retrieval ‚Üí Hybrid search with auto-tuned alpha\n3. Reranking ‚Üí Cross-encoder refinement\n4. Context Building ‚Üí Dedup, sources, length limits\n5. Prompt Engineering ‚Üí Query-type specific templates\n6. Generation ‚Üí LLM response\n7. Metadata Collection ‚Üí Timings, sources, scores\n\n**Key Metrics:**\n- **retrieval_time:** Embedding + Pinecone query (50-150ms)\n- **rerank_time:** Cross-encoder scoring (50-100ms)\n- **generation_time:** LLM response (500-2000ms)\n- **total_time:** End-to-end latency (600-2300ms)\n\n**Metadata:**\n- chunks_retrieved, avg_score, sources list\n- Query type and keywords for analytics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Complete Pipeline Demo\nfrom m1_4_query_pipeline import ProductionRAG\n\n# Initialize production RAG\nrag = ProductionRAG(\n    openai_client=openai_client,\n    pinecone_client=pinecone_client,\n    use_expansion=False,\n    use_reranking=True\n)\n\n# Run complete pipeline\ntest_query = \"How can I reduce RAG latency?\"\nprint(f\"üîç Running complete pipeline for: '{test_query}'\\\\n\")\n\nresult = rag.query(\n    query=test_query,\n    top_k=5,\n    rerank_top_k=3,\n    namespace=\"demo\",\n    temperature=0.1\n)\n\n# Display results\nprint(f\"üìä Pipeline Results:\")\nprint(f\"  ‚Ä¢ Query Type: {result['query_type']}\")\nprint(f\"  ‚Ä¢ Chunks Retrieved: {result['chunks_retrieved']}\")\nprint(f\"  ‚Ä¢ Avg Score: {result['avg_score']:.3f}\")\nprint(f\"\\\\n‚è±Ô∏è Timings:\")\nprint(f\"  ‚Ä¢ Retrieval: {result['retrieval_time']}s\")\nprint(f\"  ‚Ä¢ Generation: {result['generation_time']}s\")\nprint(f\"  ‚Ä¢ Total: {result['total_time']}s\")\nprint(f\"\\\\nüìö Sources: {', '.join(result['sources'][:3])}\")\n\n# Expected: chunks_retrieved, timings, sources comma-separated",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Common Failures, Fallbacks, Decision Card\n\n### Five Common Production Failures\n\n**1. Empty Retrieval Results**\n- **Cause:** Query too specific, no indexed content matches\n- **Fallback:** Return \"No relevant information found\" + suggest query refinement\n- **Prevention:** Monitor retrieval hit rates; add default content\n\n**2. API Timeout (Pinecone/OpenAI)**\n- **Cause:** Network issues, service degradation, rate limits\n- **Fallback:** Retry with exponential backoff (3 attempts); return cached response if available\n- **Prevention:** Set reasonable timeouts (2s retrieval, 10s generation); implement circuit breakers\n\n**3. Context Overflow**\n- **Cause:** Retrieved chunks exceed token budget\n- **Fallback:** Truncate context to max_length; prioritize highest-scoring chunks\n- **Prevention:** Smart chunking (512 tokens/chunk); enforce strict length limits\n\n**4. LLM Hallucination Despite Context**\n- **Cause:** Model extrapolates beyond provided information\n- **Fallback:** Add \"based on context only\" constraint in system prompt; log for review\n- **Prevention:** Lower temperature (0.1); use stricter prompts; post-filter responses\n\n**5. Reranker Model Load Failure**\n- **Cause:** Model file missing, memory limits, corrupted weights\n- **Fallback:** Skip reranking; return initial retrieval results\n- **Prevention:** Pre-load models at startup; monitor memory usage; graceful degradation\n\n---\n\n### Decision Card: When to Use RAG Pipelines\n\n**‚úÖ Use RAG When:**\n- Content changes frequently (docs, FAQs, knowledge bases)\n- Need source attribution for compliance/trust\n- Query diversity is high (can't pre-generate all answers)\n- Domain knowledge exceeds LLM training cutoff\n- Handling 100+ documents with evolving content\n\n**‚ùå Don't Use RAG When:**\n- Answers are static and finite (use pre-generated cache)\n- Real-time latency critical (<100ms requirement)\n- Content fits in single prompt (<4K tokens)\n- Queries are highly repetitive (use lookup table)\n- No infrastructure for vector DB + embeddings\n\n**üí∞ Cost Considerations:**\n- Embedding costs: ~$0.0001 per 1K tokens\n- Pinecone: ~$70/month (starter tier, 100K vectors)\n- LLM generation: ~$0.0005 per query (GPT-4o-mini)\n- Reranker: Free (local cross-encoder)\n\n**‚öñÔ∏è Latency Budget:**\n- No reranking: ~200-300ms\n- With reranking: ~300-400ms\n- If acceptable for your use case, RAG is viable\n\n---\n\n**Link to Previous Module:**  \nSee **M1.3 ‚Äî Indexing & Retrieval Strategies** for vector DB setup and hybrid search foundations.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}