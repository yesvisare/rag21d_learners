{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2.1 ‚Äî Caching Strategies for Cost Reduction\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Deploy a multi-layer Redis caching system reducing RAG costs by 30-70%\n",
    "- Configure cache invalidation based on content freshness requirements\n",
    "- Diagnose and resolve five common production failures\n",
    "- Recognize scenarios where caching creates more problems than solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objectives & Reality Check\n",
    "\n",
    "### What Caching Accomplishes\n",
    "- **Cost Reduction:** 30-70% for systems with repeating query patterns\n",
    "- **Latency:** ~800ms ‚Üí ~50ms for cache hits\n",
    "- **Scalability:** Handle more traffic without proportional cost increases\n",
    "\n",
    "### What Caching Cannot Do\n",
    "- Help when every question differs (>90% query diversity = <10% hit rates)\n",
    "- Guarantee data freshness; invalidation remains difficult\n",
    "- Eliminate LLM processing time on initial requests\n",
    "\n",
    "### When NOT to Use Caching\n",
    "- Query diversity exceeds 90%\n",
    "- Content updates required within 5-minute windows\n",
    "- Traffic below 500 daily queries\n",
    "- Single-server deployments with low volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token/Cost Math: Reality Check\n",
    "\n",
    "# Assumptions\n",
    "avg_tokens_per_query = 1500  # input + output\n",
    "cost_per_1k_tokens = 0.002  # GPT-3.5-turbo approximate\n",
    "queries_per_day = 10000\n",
    "cache_hit_rate = 0.50  # 50% of queries hit cache\n",
    "\n",
    "# Without caching\n",
    "daily_cost_no_cache = (queries_per_day * avg_tokens_per_query / 1000) * cost_per_1k_tokens\n",
    "\n",
    "# With caching\n",
    "cache_misses = queries_per_day * (1 - cache_hit_rate)\n",
    "daily_cost_with_cache = (cache_misses * avg_tokens_per_query / 1000) * cost_per_1k_tokens\n",
    "\n",
    "savings = daily_cost_no_cache - daily_cost_with_cache\n",
    "savings_pct = (savings / daily_cost_no_cache) * 100\n",
    "\n",
    "# Expected:\n",
    "# Without cache: $30.00/day\n",
    "# With cache (50% hit): $15.00/day\n",
    "# Savings: 50%\n",
    "\n",
    "print(f\"Without cache: ${daily_cost_no_cache:.2f}/day\")\n",
    "print(f\"With cache ({cache_hit_rate*100:.0f}% hit): ${daily_cost_with_cache:.2f}/day\")\n",
    "print(f\"Savings: {savings_pct:.0f}% (${savings:.2f}/day)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When caching FAILS: High diversity scenario\n",
    "\n",
    "# Simulate high-diversity workload\n",
    "import random\n",
    "\n",
    "unique_queries = set()\n",
    "total_queries = 1000\n",
    "\n",
    "# Generate mostly unique queries (90% diversity)\n",
    "for i in range(total_queries):\n",
    "    if random.random() < 0.90:  # 90% unique\n",
    "        unique_queries.add(f\"query_{i}\")\n",
    "    else:  # 10% repeats\n",
    "        unique_queries.add(f\"query_{random.randint(0, 100)}\")\n",
    "\n",
    "diversity = len(unique_queries) / total_queries\n",
    "theoretical_hit_rate = 1 - diversity\n",
    "\n",
    "# Expected:\n",
    "# Diversity: 90%\n",
    "# Max hit rate: 10%\n",
    "# Verdict: DON'T CACHE\n",
    "\n",
    "print(f\"Diversity: {diversity*100:.0f}%\")\n",
    "print(f\"Theoretical max hit rate: {theoretical_hit_rate*100:.0f}%\")\n",
    "print(f\"Verdict: {'‚ùå DON\\'T CACHE' if theoretical_hit_rate < 0.2 else '‚úì Cache viable'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Architecture: Multi-Layer Cache\n\n### Three-Layer Design\n\n**Layer 1: Query Cache (Exact + Semantic)**\n- Exact match via SHA-256 hash\n- Semantic match via fuzzy string similarity (BM25/MinHash)\n- Stores final LLM responses\n\n**Layer 2: Embedding Cache**\n- Caches vector embeddings (1536 dims = ~6KB each)\n- Reduces OpenAI API calls for repeated text\n- TTL: 2 hours (embeddings rarely change)\n\n**Layer 3: Retrieved-Context Cache**\n- Caches document snippets fetched from vector DB\n- Keyed by sorted document IDs\n- Multiple queries often retrieve same documents\n\n### Request Flow\n```\nQuery ‚Üí Exact Cache? ‚Üí Semantic Cache? ‚Üí Embedding Cache? ‚Üí Vector DB ‚Üí Context Cache? ‚Üí LLM ‚Üí Cache Result\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Quick architecture verification\nimport config\n\nprint(\"=== Cache Layer Configuration ===\")\nprint(f\"Exact Cache: {'‚úì' if config.ENABLE_EXACT_CACHE else '‚úó'} (TTL: {config.TTL_EXACT_CACHE}s)\")\nprint(f\"Semantic Cache: {'‚úì' if config.ENABLE_SEMANTIC_CACHE else '‚úó'} (TTL: {config.TTL_SEMANTIC_CACHE}s)\")\nprint(f\"Embedding Cache: {'‚úì' if config.ENABLE_EMBEDDING_CACHE else '‚úó'} (TTL: {config.TTL_EMBEDDING_CACHE}s)\")\nprint(f\"Context Cache: {'‚úì' if config.ENABLE_CONTEXT_CACHE else '‚úó'} (TTL: {config.TTL_CONTEXT_CACHE}s)\")\nprint(f\"\\nSemantic threshold: {config.SEMANTIC_THRESHOLD}\")\n\n# Expected:\n# All layers enabled with default TTLs\n# Semantic threshold: 0.85",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Redis Setup & Connection\n\n### Prerequisites\n- **Docker:** `docker run -d -p 6379:6379 redis:7-alpine`\n- **Redis Cloud:** Free tier at redis.com/try-free\n- **Environment:** Copy `.env.example` to `.env` and set `REDIS_URL`\n\n### Connection Testing\nTest Redis connectivity and handle graceful fallback if unavailable.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test Redis and OpenAI connections\nimport config\nfrom m2_1_caching import MultiLayerCache\n\n# Initialize clients\nredis_client = config.get_redis()\nopenai_client = config.get_openai()\n\nif redis_client:\n    print(\"‚úì Redis connected\")\n    info = redis_client.info(\"server\")\n    print(f\"  Version: {info.get('redis_version', 'unknown')}\")\nelse:\n    print(\"‚ö†Ô∏è Redis not available - will skip network-dependent cells\")\n\nif openai_client:\n    print(\"‚úì OpenAI client initialized\")\nelse:\n    print(\"‚ö†Ô∏è OpenAI not configured - will skip API calls\")\n\n# Initialize cache system\ncache = MultiLayerCache(redis_client, openai_client)\n\n# Expected:\n# ‚úì Redis connected OR ‚ö†Ô∏è Redis not available\n# ‚úì OpenAI client initialized OR ‚ö†Ô∏è OpenAI not configured",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Exact & Semantic Query Cache\n\n### How It Works\n1. **Exact Match:** SHA-256 hash of query string\n2. **Semantic Match:** Fuzzy similarity (rapidfuzz) with threshold\n3. **Response Storage:** Complete LLM response cached for reuse\n\n### Demonstration\nSimulate two similar queries to show cache hits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exact cache demonstration\nif redis_client:\n    query1 = \"How do I reset my password?\"\n    response1 = {\"answer\": \"Visit settings > security > reset password\", \"source\": \"docs\"}\n\n    # First request - MISS\n    cached = cache.get_exact(query1)\n    if not cached:\n        cache.set_exact(query1, response1)\n\n    # Second request - HIT (exact match)\n    cached = cache.get_exact(query1)\n    print(f\"Exact cache result: {cached}\")\n\n    # Expected:\n    # ‚úó Cache MISS [exact]\n    # ‚úì Cache HIT [exact]\n    # Result: {\"answer\": \"Visit settings...\", \"source\": \"docs\"}\nelse:\n    print(\"‚ö†Ô∏è Skipping (no Redis)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Semantic cache demonstration\nif redis_client:\n    query2 = \"What are your business hours?\"\n    query2_similar = \"What time are you open?\"\n    response2 = {\"answer\": \"Mon-Fri 9am-5pm EST\", \"source\": \"contact\"}\n\n    # Store original\n    cache.set_semantic(query2, response2)\n\n    # Try similar query\n    cached_semantic = cache.get_semantic(query2_similar, threshold=0.70)\n    print(f\"Semantic match found: {cached_semantic is not None}\")\n    if cached_semantic:\n        print(f\"Result: {cached_semantic}\")\n\n    # Expected:\n    # ‚úó Cache MISS [semantic] (scan finds match)\n    # ‚úì Cache HIT [semantic]\n    # Semantic match found: True\nelse:\n    print(\"‚ö†Ô∏è Skipping (no Redis)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Embedding & Context Caches\n\n### Embedding Cache\nReduces OpenAI API calls by caching vector embeddings (1536 dims = ~6KB).\nIncludes stampede protection via per-key locks.\n\n### Context Cache\nStores retrieved document snippets keyed by document IDs.\nMultiple queries often fetch the same documents from vector DB.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Embedding cache demonstration\nif redis_client and openai_client:\n    text = \"machine learning embeddings\"\n    \n    # First call - computes embedding\n    embedding1 = cache.compute_or_get_embedding(text)\n    \n    # Second call - retrieves from cache\n    embedding2 = cache.compute_or_get_embedding(text)\n    \n    if embedding1:\n        print(f\"Embedding cached: {len(embedding1)} dimensions\")\n        print(f\"Match: {embedding1 == embedding2}\")\n    \n    # Expected:\n    # ‚úó Cache MISS [embedding]\n    # ‚úì Cache HIT [embedding]\n    # Embedding cached: 1536 dimensions\n    # Match: True\nelse:\n    print(\"‚ö†Ô∏è Skipping (no OpenAI/Redis)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Context cache demonstration\nif redis_client:\n    doc_ids = [\"doc_123\", \"doc_456\", \"doc_789\"]\n    contexts = [\n        {\"id\": \"doc_123\", \"text\": \"Password reset instructions...\"},\n        {\"id\": \"doc_456\", \"text\": \"Security best practices...\"},\n        {\"id\": \"doc_789\", \"text\": \"Account recovery steps...\"}\n    ]\n    \n    # Cache contexts\n    cache.set_context(doc_ids, contexts)\n    \n    # Retrieve (different order, same docs)\n    doc_ids_reordered = [\"doc_789\", \"doc_123\", \"doc_456\"]\n    cached_contexts = cache.get_context(doc_ids_reordered)\n    \n    print(f\"Context cache hit: {cached_contexts is not None}\")\n    if cached_contexts:\n        print(f\"Retrieved {len(cached_contexts)} documents\")\n    \n    # Expected:\n    # ‚úì Cache HIT [context]\n    # Context cache hit: True\n    # Retrieved 3 documents\nelse:\n    print(\"‚ö†Ô∏è Skipping (no Redis)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Invalidation Strategies\n\n### Three Approaches\n1. **TTL (Time-To-Live):** Automatic expiration via Redis\n2. **Manual Bust:** Explicit invalidation when content updates\n3. **Stale Detection:** Timestamp-based freshness policies\n\n### Reality Check\nInvalidation remains genuinely difficult. Choose conservative TTLs and monitor stale data incidents.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Invalidation demonstrations\nif redis_client:\n    # 1. Manual query invalidation\n    query_to_bust = \"How do I reset my password?\"\n    cache.invalidate_query(query_to_bust)\n    print(\"‚úì Invalidated specific query\")\n    \n    # 2. Prefix-based invalidation (clear all semantic cache)\n    count = cache.invalidate_by_prefix(config.PREFIX_SEMANTIC)\n    print(f\"‚úì Cleared semantic cache: {count} keys\")\n    \n    # 3. Stale data cleanup (entries older than 1 hour)\n    cache.invalidate_stale(max_age_seconds=3600)\n    \n    # Expected:\n    # ‚úì Invalidated specific query\n    # ‚úì Cleared semantic cache: N keys\n    # üóëÔ∏è Invalidated N stale entries\nelse:\n    print(\"‚ö†Ô∏è Skipping (no Redis)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Common Failures & Fixes\n\n### Five Production Issues\n1. **Cache Stampede:** Concurrent requests overwhelm backend\n2. **Stale Data:** Updates not reflected until TTL expires\n3. **Memory Exhaustion:** Large embeddings fill Redis\n4. **Hash Collisions:** Wrong results from weak hashes\n5. **Low ROI:** <20% hit rate wastes infrastructure costs\n\n### Solutions Demonstrated Below",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate cache metrics and ROI check\nprint(\"=== Cache Performance Metrics ===\")\nprint(cache.metrics.summary())\n\n# Check if caching is worthwhile\nhit_rate = cache.metrics.get_hit_rate()\nif hit_rate < 20:\n    print(f\"\\n‚ö†Ô∏è WARNING: Hit rate {hit_rate:.1f}% too low - consider disabling cache\")\nelse:\n    print(f\"\\n‚úì Hit rate {hit_rate:.1f}% - caching provides value\")\n\n# Expected output varies based on previous cells\n# Example: Hits: 5, Misses: 3, Hit Rate: 62.5%",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Decision Card & Cost Projection\n\n### Decision Matrix\n\n| Scenario | Daily Queries | Diversity | Freshness Need | Recommendation |\n|----------|---------------|-----------|----------------|----------------|\n| FAQ Bot | 5,000+ | <30% | >1 hour | ‚úì Cache |\n| News Search | 10,000+ | >90% | <5 min | ‚úó Don't Cache |\n| Support Docs | 2,000+ | 40% | >30 min | ‚úì Cache |\n| Research Q&A | 500 | 85% | Any | ‚úó Don't Cache |\n\n### Cost Projection Calculator",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Interactive cost projection\ndef project_costs(queries_per_day, hit_rate_pct, avg_tokens=1500, cost_per_1k=0.002):\n    \"\"\"Project costs with and without caching.\"\"\"\n    hit_rate = hit_rate_pct / 100\n    \n    # Without cache\n    cost_no_cache = (queries_per_day * avg_tokens / 1000) * cost_per_1k\n    \n    # With cache\n    cache_misses = queries_per_day * (1 - hit_rate)\n    cost_with_cache = (cache_misses * avg_tokens / 1000) * cost_per_1k\n    \n    # Redis costs (estimate $10/month for basic tier)\n    redis_monthly = 10\n    redis_daily = redis_monthly / 30\n    \n    total_with_cache = cost_with_cache + redis_daily\n    savings = cost_no_cache - total_with_cache\n    monthly_savings = savings * 30\n    \n    return {\n        \"no_cache\": cost_no_cache,\n        \"with_cache_llm\": cost_with_cache,\n        \"redis_daily\": redis_daily,\n        \"total_with_cache\": total_with_cache,\n        \"daily_savings\": savings,\n        \"monthly_savings\": monthly_savings,\n        \"roi\": (savings / redis_daily * 100) if redis_daily > 0 else 0\n    }\n\n# Example scenarios\nscenarios = [\n    (\"FAQ Bot (good fit)\", 5000, 60),\n    (\"Support Docs (moderate)\", 2000, 40),\n    (\"High Diversity (poor fit)\", 10000, 10)\n]\n\nfor name, queries, hit_rate in scenarios:\n    result = project_costs(queries, hit_rate)\n    print(f\"\\n{name}\")\n    print(f\"  Queries/day: {queries:,}, Hit rate: {hit_rate}%\")\n    print(f\"  Daily: ${result['no_cache']:.2f} ‚Üí ${result['total_with_cache']:.2f}\")\n    print(f\"  Monthly savings: ${result['monthly_savings']:.2f}\")\n    print(f\"  Verdict: {'‚úì Deploy' if result['daily_savings'] > 0 else '‚úó Skip'}\")\n\n# Expected:\n# FAQ Bot: Significant savings\n# Support Docs: Moderate savings  \n# High Diversity: Minimal/negative ROI",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}