# M1.4 â€” Query Pipeline & Response Generation

**Complete 7-Stage RAG Pipeline:**
`Query â†’ Retrieval â†’ Rerank â†’ Context â†’ LLM â†’ Answer`

---

## Purpose

Transform user queries into grounded, cited answers through a production-ready 7-stage RAG pipeline. This module integrates query understanding, hybrid retrieval, cross-encoder reranking, context preparation, and LLM generation with comprehensive metrics tracking to reduce hallucination by 60-80%.

## Concepts Covered

- **Query Classification**: 6 types (factual, how-to, comparison, definition, troubleshooting, opinion)
- **Query Expansion**: LLM-based alternative phrasings for 15-25% recall improvement
- **Hybrid Retrieval**: Dense (semantic) + sparse (BM25) with auto-tuned alpha (0.3-0.8)
- **Cross-Encoder Reranking**: ms-marco-MiniLM-L-6-v2 for 10-20% relevance gain
- **Context Preparation**: Deduplication, source attribution, length guards
- **Prompt Engineering**: Query-type specific templates for optimal responses
- **Response Generation**: Streaming and non-streaming modes
- **Metrics & Attribution**: Timings, scores, source tracking

## After Completing This Module

You will be able to:
- âœ… Build end-to-end RAG pipelines with 7 sequential stages
- âœ… Implement hybrid search combining semantic and keyword retrieval
- âœ… Apply query-type specific optimizations (alpha tuning 0.3-0.8)
- âœ… Use cross-encoder reranking to improve result quality
- âœ… Handle graceful fallbacks for missing API keys and service failures
- âœ… Track comprehensive metrics (retrieval_time, generation_time, avg_score)
- âœ… Provide source attribution for compliance and trust
- âœ… Make informed trade-offs (accuracy vs latency, coverage vs precision)

## Context in RAG21D Track

**Prerequisites**: M1.3 (Indexing & Retrieval Strategies)
**Builds on**: Vector DB setup, hybrid search foundations
**Adds**: Query understanding, reranking, production metrics
**Enables**: Production RAG deployments with 60-80% hallucination reduction

---

## ğŸ“ Project Structure

```
rag21d_learners/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ .gitignore                         # Git ignore patterns
â”œâ”€â”€ .env.example                       # API key template
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ app.py                             # FastAPI application (thin wrapper)
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ m1_4_query_pipeline/
â”‚       â”œâ”€â”€ __init__.py               # Package init with learning arc
â”‚       â”œâ”€â”€ config.py                 # Environment & client configuration
â”‚       â”œâ”€â”€ module.py                 # Core pipeline components (7 stages)
â”‚       â””â”€â”€ router.py                 # FastAPI endpoints
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ M1_4_Query_Pipeline_and_Response.ipynb  # Interactive tutorial
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_query_pipeline.py        # Component smoke tests
â”‚   â””â”€â”€ test_smoke.py                 # API endpoint tests
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ example/
â”‚       â””â”€â”€ example_data.txt          # 20 sample documents
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ run_local.ps1                 # Windows run script
```

---

## ğŸš€ Quickstart

### 1. Setup Environment

```bash
# Install dependencies
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env with your keys:
#   OPENAI_API_KEY=sk-...
#   PINECONE_API_KEY=pc-...
#   PINECONE_REGION=us-east-1
```

### 2. Run API Server

**Windows (PowerShell):**
```powershell
.\scripts\run_local.ps1
```

**Windows (Alternative):**
```powershell
$env:PYTHONPATH="$PWD"
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

**Linux/Mac:**
```bash
export PYTHONPATH=$PWD
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

**Access:**
- API: http://localhost:8000
- Interactive docs: http://localhost:8000/docs
- Redoc: http://localhost:8000/redoc

### 3. Run Notebook

```bash
jupyter notebook notebooks/M1_4_Query_Pipeline_and_Response.ipynb
```

### 4. CLI Usage

**Module Interface:**
```bash
# Basic query
python -m src.m1_4_query_pipeline.module --ask "How do I improve RAG accuracy?"

# Streaming query
python -m src.m1_4_query_pipeline.module --stream "Optimize Pinecone index"

# Without reranking (faster)
python -m src.m1_4_query_pipeline.module --ask "What is RAG?" --rerank 0
```

---

## ğŸŒ API Usage

### Example: Query with curl

```bash
curl -X POST "http://localhost:8000/m1_4_query_pipeline/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "How do I improve RAG accuracy?",
    "top_k": 5,
    "rerank_top_k": 3
  }'
```

### Example: Python Requests

```python
import requests

response = requests.post(
    "http://localhost:8000/m1_4_query_pipeline/query",
    json={"query": "How do I improve RAG accuracy?", "top_k": 5}
)

result = response.json()
print(f"Answer: {result['answer']}")
print(f"Total time: {result['total_time']}s")
```

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Module information |
| `/m1_4_query_pipeline/health` | GET | Health check |
| `/m1_4_query_pipeline/query` | POST | Execute pipeline |
| `/m1_4_query_pipeline/metrics` | GET | Get metrics |

---

## ğŸ’° Costs & Latency

**Cost per 1000 queries:** ~$0.60
**Latency:** 600-2300ms (with/without reranking)

See full cost/latency breakdown in the docs above.

---

## ğŸ§ª Testing

```bash
export PYTHONPATH=$PWD  # or $env:PYTHONPATH="$PWD" on Windows
pytest tests/ -v
```

---

**Built with:** OpenAI, Pinecone, Sentence-Transformers, FastAPI
**License:** MIT
**Module:** M1.4 of RAG21D Learner Series
