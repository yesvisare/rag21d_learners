{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# M1.3 ‚Äî Document Processing Pipeline\n\n**Extraction ‚Üí Cleaning ‚Üí Chunking ‚Üí Embedding ‚Üí Storage**\n\n## Purpose\nTransform raw documents (PDF, TXT, Markdown) into searchable vector embeddings for production RAG systems. This notebook teaches the complete pipeline from document extraction to Pinecone storage, with hands-on examples of each stage.\n\n## Concepts Covered\n- Multi-format document extraction (PDF, TXT, Markdown)\n- Text cleaning and Unicode normalization\n- Three chunking strategies (fixed, semantic, paragraph-aware)\n- Metadata enrichment for precise retrieval filtering\n- Batch embedding generation with OpenAI\n- Vector storage in Pinecone with size limits and deduplication\n\n## After Completing This Module\nYou will be able to:\n- Build production document processing pipelines\n- Choose appropriate chunking strategies for your use case\n- Handle common failures (Unicode errors, memory issues, bad chunking)\n- Make informed decisions between custom pipelines vs managed services\n- Cost-effectively process documents at scale (~$0.13 per million tokens)\n\n## Context in Track\n- **M1.1**: Vector database fundamentals\n- **M1.2**: Pinecone data model and indexing\n- **M1.3** (this module): Document processing pipeline\n- **M1.4** (next): Query pipeline and response generation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Document Processing Matters (Reality Check & Trade-offs)\n",
    "\n",
    "Document processing transforms raw text into searchable embeddings. Traditional keyword search fails when users query \"machine learning algorithms\" but documents say \"ML techniques\". Semantic search bridges this gap.\n",
    "\n",
    "### What You Gain\n",
    "- **Semantic search**: Find content by meaning, not keywords\n",
    "- **Automation**: Process hundreds of docs without manual tagging\n",
    "- **Scale**: Handle diverse formats (PDF, TXT, MD)\n",
    "\n",
    "### What You Lose\n",
    "- **Exact formatting**: Tables and images become text soup\n",
    "- **Cross-doc reasoning**: No knowledge graph linking\n",
    "- **Speed**: Embedding 1000 chunks takes ~30 seconds\n",
    "\n",
    "### When NOT to Build This\n",
    "- **<10 documents**: Manual processing may be faster\n",
    "- **Scanned PDFs**: Need OCR first (Tesseract, AWS Textract)\n",
    "- **Real-time updates**: Reprocessing takes time\n",
    "- **<100K token docs**: Long-context embeddings preserve full context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick demo: raw text ‚Üí cleaned text transformation\nfrom src.m1_3_document_processing import TextCleaner\n\nraw_text = 'He said \"hello\" and\\nsh e   said   \"goodbye\".   \\n\\n\\n\\nEnd.'\ncleaner = TextCleaner()\ncleaned = cleaner.clean(raw_text)\n\nprint(f\"Raw ({len(raw_text)} chars): {repr(raw_text)[:60]}...\")\nprint(f\"Cleaned ({len(cleaned)} chars): {repr(cleaned)[:60]}...\")\n\n# Expected:\n# - Multiple spaces ‚Üí single space\n# - Multiple newlines ‚Üí double newlines"
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Extraction (PDF/TXT/Markdown) + Metadata\n\nThe `DocumentExtractor` pulls text from multiple formats and preserves metadata:\n\n- **PDF**: PyMuPDF extracts text page-by-page\n- **TXT/MD**: Plain text readers with encoding fallback\n- **Metadata**: File name, size, page count, extraction method\n\n### Key Features\n- Generates unique `doc_id` (hash of filepath)\n- Handles Unicode errors gracefully (latin-1 fallback)\n- Raises clear errors for unsupported file types",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract document and show metadata\nfrom src.m1_3_document_processing import DocumentExtractor\n\nextractor = DocumentExtractor()\ndoc = extractor.extract('data/example/example_data.txt')\n\nprint(f\"doc_id: {doc.doc_id}\")\nprint(f\"text length: {len(doc.text)} chars\")\nprint(f\"metadata keys: {list(doc.metadata.keys())}\")\n\n# Expected:\n# - doc_id: 12-char hash\n# - text length: ~1500 chars\n# - metadata: file_name, file_type, page_count, etc.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Cleaning & Normalization (Artifacts, Unicode, Line Breaks)\n\nRaw extracted text contains noise that degrades embedding quality:\n\n- **Unicode issues**: Smart quotes (`\"\"`), em dashes (`‚Äî`), special chars\n- **Whitespace chaos**: Multiple spaces, tabs, excessive newlines\n- **PDF artifacts**: Hyphenated line breaks (`exam-\\nple`), page numbers\n- **Format remnants**: Headers, footers, metadata strings\n\nThe `TextCleaner` normalizes text using:\n- **NFKC Unicode normalization**: Standardizes characters\n- **Punctuation mapping**: Smart quotes ‚Üí standard quotes\n- **Regex patterns**: Fix hyphenated words, collapse whitespace\n- **Artifact removal**: Strip common page numbers and headers",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Clean the extracted document text\nfrom src.m1_3_document_processing import TextCleaner\n\ncleaner = TextCleaner()\ncleaned_text = cleaner.clean(doc.text)\n\nprint(f\"Before: {len(doc.text)} chars\")\nprint(f\"After: {len(cleaned_text)} chars\")\nprint(f\"First 120 chars: {cleaned_text[:120]}...\")\n\n# Expected:\n# - Slightly shorter (whitespace removed)\n# - No excessive newlines or spaces\n# - Unicode normalized",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Chunking Strategies (Fixed, Semantic, Paragraph-Aware)\n\nChunking splits documents into retrievable units. The strategy affects:\n- **Retrieval precision**: Small chunks = specific matches\n- **Context preservation**: Large chunks = more context\n- **Cost**: More chunks = more embeddings\n\n### Three Strategies\n\n**Fixed-Size Chunking**\n- Splits at character boundaries (e.g., 512 chars)\n- Fast, predictable\n- May split mid-sentence\n\n**Semantic Chunking**\n- Recursive split: paragraphs ‚Üí sentences ‚Üí words\n- Respects boundaries, preserves meaning\n- Slower, variable chunk sizes\n\n**Paragraph Chunking**\n- Groups paragraphs up to max size\n- Best for structured documents\n- Requires clear paragraph markers",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare all three chunking strategies\nfrom src.m1_3_document_processing import FixedSizeChunker, SemanticChunker, ParagraphChunker\n\nfixed = FixedSizeChunker(chunk_size=300, overlap=30)\nsemantic = SemanticChunker(chunk_size=300, overlap=30)\nparagraph = ParagraphChunker(max_chunk_size=600)\n\nfixed_chunks = fixed.chunk(cleaned_text)\nsemantic_chunks = semantic.chunk(cleaned_text)\nparagraph_chunks = paragraph.chunk(cleaned_text)\n\nprint(f\"Fixed: {len(fixed_chunks)} chunks | First chunk ends: ...{fixed_chunks[0][-80:]}\")\nprint(f\"Semantic: {len(semantic_chunks)} chunks | First chunk ends: ...{semantic_chunks[0][-80:]}\")\nprint(f\"Paragraph: {len(paragraph_chunks)} chunks | First chunk ends: ...{paragraph_chunks[0][-80:]}\")\n\n# Expected:\n# - Different chunk counts (fixed ‚âà semantic > paragraph)\n# - Semantic/paragraph end at better boundaries",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Metadata Enrichment (IDs, Hash, Counts, Semantic Flags)\n\nRich metadata enables powerful filtering during retrieval. Without metadata, you retrieve chunks blindly. With it, you can:\n- **Filter by content type**: Skip code chunks for user docs\n- **Detect duplicates**: Check `content_hash` before upserting\n- **Debug retrieval**: Trace chunks back to source documents\n\n### Metadata Fields\n\n**Identifiers**\n- `chunk_id`: Unique ID (file_chunk_index_hash)\n- `content_hash`: MD5 of chunk text (deduplication)\n- `doc_id`: Parent document ID\n\n**Counts**\n- `word_count`, `char_count`, `line_count`: Size metrics\n\n**Semantic Flags**\n- `contains_code`: Code snippets (def, function, class, etc.)\n- `is_list`: Bulleted/numbered lists (>50% list markers)\n- `has_heading`: Starts with Markdown heading or all-caps title",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract metadata for first semantic chunk\nfrom src.m1_3_document_processing import MetadataExtractor\n\nmetadata_extractor = MetadataExtractor()\nfirst_chunk_metadata = metadata_extractor.extract(semantic_chunks[0], doc.metadata, chunk_index=0)\n\n# Show subset of metadata\nprint({\n    'chunk_id': first_chunk_metadata['chunk_id'],\n    'word_count': first_chunk_metadata['word_count'],\n    'contains_code': first_chunk_metadata['contains_code'],\n    'is_list': first_chunk_metadata['is_list']\n})\n\n# Expected:\n# - chunk_id: example_data.txt_0_<hash>\n# - word_count: ~50-100\n# - contains_code: False (first chunk is title/intro)\n# - is_list: False",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Embeddings & Storage in Pinecone (Batches, Size Limits, Failures)\n\nThe final stage converts text chunks into dense vector embeddings and stores them in Pinecone for fast retrieval.\n\n### Embedding Generation\n- **Model**: OpenAI `text-embedding-3-small` (1536 dimensions)\n- **Cost**: ~$0.13 per million tokens\n- **Batch size**: 100 chunks per API call (faster, cheaper)\n- **Latency**: ~1 second per batch of 100 chunks\n\n### Pinecone Storage\n- **Upsert limit**: 100 vectors per request\n- **Metadata limit**: 40KB per vector (we trim if exceeded)\n- **Namespaces**: Logical partitions within an index\n\n### Common Failures\n- **Metadata too large**: Trim fields exceeding 30KB\n- **Rate limits**: Batch requests with retry logic\n- **Duplicate IDs**: Use `content_hash` for deduplication",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prepare chunks with metadata, then embed if API keys available\nfrom src.m1_3_document_processing import Chunk, EmbeddingPipeline\nfrom src.m1_3_document_processing.config import get_clients\n\n# Create Chunk objects with enriched metadata\nchunks_with_metadata = []\nfor i, chunk_text in enumerate(semantic_chunks[:3]):  # Just first 3 for demo\n    metadata = metadata_extractor.extract(chunk_text, doc.metadata, i)\n    chunks_with_metadata.append(Chunk(chunk_id=metadata['chunk_id'], text=chunk_text, metadata=metadata))\n\n# Get API clients (may be None if keys not configured)\nopenai_client, pinecone_client = get_clients()\n\n# Create pipeline and attempt embedding\npipeline = EmbeddingPipeline(openai_client, pinecone_client, 'demo-index')\n\nif openai_client:\n    try:\n        vectors = pipeline.embed_chunks(chunks_with_metadata)\n        print(f\"Generated {len(vectors)} embeddings\")\n        print(f\"Vector shape: id={vectors[0]['id']}, values=[{len(vectors[0]['values'])} dims], metadata keys={list(vectors[0]['metadata'].keys())}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Skipping embedding (API error): {e}\")\nelse:\n    # Show payload shape without calling API\n    print(\"‚ö†Ô∏è Skipping API calls (no keys found)\")\n    print(f\"Would generate {len(chunks_with_metadata)} vectors\")\n    print(f\"Payload shape: {{'id': 'chunk_id', 'values': [1536 floats], 'metadata': {list(chunks_with_metadata[0].metadata.keys())}}}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Common Failures & Fixes + Decision Card\n\n### Five Common Failure Modes\n\n**1. Unicode Errors (ÔøΩ characters)**\n- **Cause**: Smart quotes, em dashes, special chars not handled\n- **Fix**: `TextCleaner` uses NFKC normalization + punctuation mapping\n- **Prevention**: Test with diverse PDFs (academic papers, scanned docs)\n\n**2. Memory Exhaustion**\n- **Cause**: 500MB PDF loaded entirely into memory\n- **Fix**: Process page-by-page, use streaming readers\n- **Prevention**: Add file size checks, limit max pages\n\n**3. Bad Chunking (Code Split Mid-Function)**\n- **Cause**: Fixed-size chunker splits at arbitrary boundaries\n- **Fix**: Use `contains_code` metadata flag + custom code-aware splitter\n- **Prevention**: Validate chunks on sample docs before production\n\n**4. Garbled Table Extraction**\n- **Cause**: PyMuPDF converts tables to unstructured text\n- **Fix**: Use `pdfplumber` or Camelot for table-specific extraction\n- **Prevention**: Test on docs with tables, consider hybrid approach\n\n**5. Duplicate Chunks**\n- **Cause**: Reprocessing same document without deduplication\n- **Fix**: Check `content_hash` before upserting to Pinecone\n- **Prevention**: Maintain processing logs with doc_id + timestamps\n\n---\n\n### Decision Card: When to Use Each Approach\n\n| Scenario | Best Choice | Reason |\\n|----------|-------------|--------|\\n| **50+ consistent PDFs** (e.g., all technical papers) | Custom Pipeline | Domain-specific chunking, full control |\\n| **<10 documents** | Manual Processing | Faster than building automation |\\n| **Diverse formats** (DOCX, images, scans) | Managed Service (Unstructured.io) | OCR, table extraction, multi-format support |\\n| **<100K token docs** | Long-Context Embeddings (Voyage AI) | No chunking loss, entire doc context |\\n| **Real-time updates** | Streaming Pipeline (Kafka + Lambda) | Incremental processing, not batch |\\n| **Scanned PDFs** | AWS Textract ‚Üí Custom Pipeline | OCR first, then process |\\n\\n---\n\n### Next Steps\n\n1. **Run tests**: `python tests_processing.py` (no API keys required)\\n2. **Process example doc**: `python m1_3_document_processing.py --process example_data.txt`\\n3. **Try different chunkers**: Compare quality on your own documents\\n4. **Monitor costs**: Track OpenAI token usage (embeddings add up fast)\\n5. **Validate retrieval**: In M1.4, you'll query these chunks and evaluate quality\\n\\n**Remember**: Perfect chunking is impossible. Start simple (semantic chunking), measure retrieval quality, then optimize only if needed.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Summary & Next Steps\n\n### What You've Learned\n‚úÖ Extract text from PDFs, TXT, and Markdown with metadata preservation  \n‚úÖ Clean text by normalizing Unicode, removing artifacts, fixing line breaks  \n‚úÖ Choose chunking strategies: fixed (fast), semantic (meaning-preserving), paragraph (structured)  \n‚úÖ Enrich chunks with metadata for filtering and deduplication  \n‚úÖ Generate embeddings in batches and store in Pinecone  \n‚úÖ Handle common failures and make informed architecture decisions  \n\n### Key Takeaways\n- **No perfect chunking**: Start simple (semantic), measure retrieval quality, optimize if needed\n- **Cost vs quality**: Smaller chunks = precise retrieval but more embeddings ($$$)\n- **Metadata matters**: Enables filtering by content type, deduplication, debugging\n- **Graceful degradation**: Pipeline works without API keys (returns metadata only)\n\n### Practice Exercises\n1. Process your own documents with different chunking strategies\n2. Add custom metadata extractors for your domain (e.g., extract dates, authors)\n3. Implement deduplication logic using `content_hash`\n4. Test with edge cases (large PDFs, Unicode-heavy docs, tables)\n\n### Next Module: M1.4 Query Pipeline & Response Generation\nYou'll learn to:\n- Query Pinecone with semantic search\n- Implement hybrid search (semantic + keyword)\n- Rank and filter results using metadata\n- Generate responses with retrieved context\n- Handle retrieval failures and edge cases\n\n### CLI Quick Reference\n```bash\n# Process single document\npython -m src.m1_3_document_processing.module --process data/example/example_data.txt --index production-rag\n\n# Batch processing\npython -m src.m1_3_document_processing.module --process-batch docs/ --index production-rag --chunker semantic\n\n# Start API server\npowershell -c \"$env:PYTHONPATH='$PWD'; uvicorn app:app --reload\"\n```\n\n### Resources\n- [OpenAI Embeddings Pricing](https://openai.com/pricing)\n- [Pinecone Documentation](https://docs.pinecone.io/)\n- [PyMuPDF (fitz) Docs](https://pymupdf.readthedocs.io/)\n\n**Happy processing! üöÄ**",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}