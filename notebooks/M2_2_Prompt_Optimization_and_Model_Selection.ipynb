{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# M2.2 ‚Äî Prompt Optimization & Model Selection\n\n## Purpose\n\nLearn to reduce RAG LLM costs by 30-50% through intelligent prompt engineering, token optimization, and model routing **without sacrificing quality**. This module teaches you when and how to optimize prompts, and critically, **when NOT to optimize**.\n\n## Concepts Covered\n\n- **RAG-specific prompt templates** (5 production-tested variants)\n- **Token estimation and cost projection** across models\n- **Intelligent model routing** based on query complexity\n- **Context formatting** and smart document truncation\n- **A/B testing framework** for prompt comparison\n- **Cost/quality trade-offs** and decision frameworks\n- **Common failure modes** and debugging strategies\n- **ROI analysis** and break-even calculations\n\n## After Completing\n\nYou will be able to:\n- Design and test prompt variants that reduce token usage by 30-50%\n- Route queries to appropriate models based on complexity and cost constraints\n- Measure and project costs at different scales (100 to 100K queries/day)\n- Identify when prompt optimization is counterproductive\n- Debug the 5 most common prompt optimization failures\n- Make data-driven decisions using ROI and decision frameworks\n\n## Context in Track\n\nThis is **Module 2.2** in the RAG Production Engineering track:\n- M1.x: Built foundational RAG system with vector search and generation\n- M2.1: Implemented caching strategies for cost reduction\n- **M2.2: Optimize prompts and route models intelligently** ‚Üê YOU ARE HERE\n- M2.3: Build production monitoring dashboards\n- M2.4: Implement error handling and reliability patterns\n\n**Prerequisites:** M2.1 (Caching), working RAG system, OpenAI API access (optional for testing)  \n**Estimated time:** 60-90 minutes for implementation + practice\n\n---\n\n**Reality Check:** Prompt optimization trades verbosity for cost. Not suitable for all use cases."
  },
  {
   "cell_type": "markdown",
   "id": "section1-title",
   "metadata": {},
   "source": [
    "## 1. Prerequisite Check & Reality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prereq-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Checking prerequisites...\\n\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "assert sys.version_info >= (3, 9), \"Python 3.9+ required\"\n",
    "\n",
    "# Check required packages\n",
    "try:\n",
    "    import openai\n",
    "    print(f\"‚úì OpenAI: {openai.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚úó OpenAI not installed. Run: pip install -r requirements.txt\")\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    print(\"‚úì Tiktoken: OK\")\n",
    "except ImportError:\n",
    "    print(\"‚úó Tiktoken not installed\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"‚úì Pandas & NumPy: OK\")\n",
    "except ImportError:\n",
    "    print(\"‚úó Data analysis packages missing\")\n",
    "\n",
    "# Check API key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if api_key and api_key.startswith(\"sk-\"):\n",
    "    print(f\"‚úì API Key: {api_key[:10]}...\")\n",
    "    HAS_API_KEY = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No API key found. Will run in DRY RUN mode (estimates only)\")\n",
    "    HAS_API_KEY = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Prerequisites check complete!\")\n",
    "if not HAS_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  Running without API key - all tests will use estimates\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reality-check",
   "metadata": {},
   "source": [
    "### Reality Check: What Prompt Optimization Actually Does\n",
    "\n",
    "**‚úÖ What it DOES well:**\n",
    "- Reduces token usage 30-50% (measured across production systems)\n",
    "- Cuts API costs proportionally (if spending $500/mo ‚Üí $250-350/mo)\n",
    "- Improves response latency 10-20% (fewer tokens = faster generation)\n",
    "\n",
    "**‚ùå What it DOESN'T do:**\n",
    "- Cannot fix poor retrieval quality (garbage in = garbage out)\n",
    "- Won't improve response quality beyond baseline (trades verbosity for conciseness)\n",
    "- Doesn't solve scaling bottlenecks (DB queries, network latency unaffected)\n",
    "\n",
    "**‚ö†Ô∏è The Trade-offs:**\n",
    "- You gain **cost savings** but risk **response quality degradation**\n",
    "- Works for **high-volume simple queries** but not **complex reasoning tasks**\n",
    "- Saves money in **production** but adds **development/monitoring overhead**\n",
    "\n",
    "**Cost structure honesty:**\n",
    "- Initial: 4-8 hours implementation\n",
    "- Ongoing: 2-4 hours/month monitoring\n",
    "- Hidden: Need A/B testing infrastructure\n",
    "\n",
    "**When NOT to use:**\n",
    "- Query volume <100/day (overhead exceeds savings)\n",
    "- Quality is non-negotiable (medical, legal, financial)\n",
    "- Query diversity >90% (caching ineffective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-math",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost comparison example\n",
    "print(\"Token Cost Reality Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scenario parameters\n",
    "queries_per_day = 10_000\n",
    "price_per_1m_tokens = 3.00  # Average blended rate\n",
    "\n",
    "# Bad prompt\n",
    "bad_tokens = 550  # 350 in + 200 out\n",
    "bad_cost_per_query = (bad_tokens / 1_000_000) * price_per_1m_tokens\n",
    "bad_daily = bad_cost_per_query * queries_per_day\n",
    "bad_monthly = bad_daily * 30\n",
    "\n",
    "# Optimized prompt\n",
    "opt_tokens = 330  # 180 in + 150 out\n",
    "opt_cost_per_query = (opt_tokens / 1_000_000) * price_per_1m_tokens\n",
    "opt_daily = opt_cost_per_query * queries_per_day\n",
    "opt_monthly = opt_daily * 30\n",
    "\n",
    "# Calculate savings\n",
    "savings_monthly = bad_monthly - opt_monthly\n",
    "savings_pct = (savings_monthly / bad_monthly) * 100\n",
    "\n",
    "print(f\"\\nBad Prompt ({bad_tokens} tokens):\")\n",
    "print(f\"  ${bad_cost_per_query:.6f} per query\")\n",
    "print(f\"  ${bad_daily:.2f}/day\")\n",
    "print(f\"  ${bad_monthly:.2f}/month\")\n",
    "\n",
    "print(f\"\\nOptimized Prompt ({opt_tokens} tokens):\")\n",
    "print(f\"  ${opt_cost_per_query:.6f} per query\")\n",
    "print(f\"  ${opt_daily:.2f}/day\")\n",
    "print(f\"  ${opt_monthly:.2f}/month\")\n",
    "\n",
    "print(f\"\\nüí∞ Savings: ${savings_monthly:.2f}/month ({savings_pct:.1f}% reduction)\")\n",
    "print(\"\\nJust from optimizing prompts!\")\n",
    "\n",
    "# Expected: ~$198/month savings, 40% reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b0umctsk",
   "source": "## 2. RAG Prompt Library\n\nExplore different prompt templates optimized for various use cases. Each template has different token counts and trade-offs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o91zz68rzsp",
   "source": "# Import our prompt optimization module\nimport sys\nimport os\n# Add project root to path\nsys.path.insert(0, os.path.abspath('..'))\n\nfrom src.m2_2_prompt_optimization import RAGPromptLibrary, TokenEstimator\nimport json\n\n# Initialize token estimator\nestimator = TokenEstimator()\n\nprint(\"RAG Prompt Template Library\")\nprint(\"=\"*60)\nprint(\"\\nAvailable Templates:\\n\")\n\n# Get all templates\ntemplates = [\n    (\"BASIC_RAG\", RAGPromptLibrary.BASIC_RAG),\n    (\"CONCISE_RAG\", RAGPromptLibrary.CONCISE_RAG),\n    (\"STRUCTURED_RAG\", RAGPromptLibrary.STRUCTURED_RAG),\n    (\"JSON_RAG\", RAGPromptLibrary.JSON_RAG),\n    (\"SUPPORT_RAG\", RAGPromptLibrary.SUPPORT_RAG),\n]\n\nfor name, template in templates:\n    # Calculate actual token count for system prompt\n    sys_tokens = estimator.count_tokens(template.system_prompt)\n    \n    print(f\"{name}:\")\n    print(f\"  Use case: {template.use_case}\")\n    print(f\"  Estimated tokens: {template.tokens_estimate}\")\n    print(f\"  System prompt tokens: {sys_tokens}\")\n    print(f\"  System prompt: {template.system_prompt[:80]}...\")\n    print()\n\n# Expected: Shows 5 templates with token estimates",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e5n439ngfd4",
   "source": "# Compare token savings across templates\nprint(\"Token Savings Comparison\")\nprint(\"=\"*60)\n\nbaseline = RAGPromptLibrary.BASIC_RAG.tokens_estimate\n\nfor name, template in templates:\n    tokens = template.tokens_estimate\n    savings = baseline - tokens\n    savings_pct = (savings / baseline) * 100 if baseline > 0 else 0\n    \n    print(f\"{name:20s} {tokens:4d} tokens  \", end=\"\")\n    if savings > 0:\n        print(f\"‚Üì {savings:3d} ({savings_pct:5.1f}% savings)\")\n    else:\n        print(\"(baseline)\")\n\nprint(\"\\nüí° Key insight: Optimization can reduce tokens by up to 60%\")\nprint(\"‚ö†Ô∏è  But may reduce response quality - always A/B test!\")\n\n# Expected: Table showing token reduction from baseline",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qz332e1i1lk",
   "source": "## 3. Context Formatting for Fewer Tokens\n\nLearn how to format retrieved documents efficiently to minimize token usage while preserving critical information.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "i08mx84ffc",
   "source": "# Load example documents\ndata_path = \"../data/example/example_data.json\"\nwith open(data_path, \"r\") as f:\n    data = json.load(f)\n\ndocuments = data[\"documents\"]\n\nprint(\"Document Context Formatting\")\nprint(\"=\"*60)\nprint(f\"\\nOriginal documents: {len(documents)}\")\n\n# Calculate original token count\noriginal_context = \"\\n\\n\".join([f\"[{i+1}] {doc['content']}\" for i, doc in enumerate(documents)])\noriginal_tokens = estimator.count_tokens(original_context)\n\nprint(f\"Original context tokens: {original_tokens}\")\nprint(f\"\\nOriginal context preview:\")\nprint(original_context[:200] + \"...\\n\")\n\n# Expected: Shows raw documents and token count",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w94p1mdolz8",
   "source": "# Test format_context_optimally with different token limits\nfrom src.m2_2_prompt_optimization import format_context_optimally\n\nprint(\"Optimized Context Formatting\")\nprint(\"=\"*60)\n\n# Test different max_tokens settings\ntest_limits = [500, 300, 150]\n\nfor max_tokens in test_limits:\n    formatted = format_context_optimally(\n        documents,\n        max_tokens=max_tokens,\n        include_metadata=False,\n        estimator=estimator\n    )\n    \n    actual_tokens = estimator.count_tokens(formatted)\n    savings = original_tokens - actual_tokens\n    savings_pct = (savings / original_tokens * 100) if original_tokens > 0 else 0\n    \n    print(f\"\\nMax tokens: {max_tokens}\")\n    print(f\"  Actual tokens: {actual_tokens}\")\n    print(f\"  Savings: {savings} tokens ({savings_pct:.1f}%)\")\n    print(f\"  Preview: {formatted[:100]}...\")\n\nprint(\"\\nüí° Smart truncation preserves most relevant docs first\")\n\n# Expected: Shows different truncation levels and token savings",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "l2d4bfxelim",
   "source": "## 4. Model Routing\n\nIntelligently route queries to appropriate models based on complexity. Simple queries use fast/cheap models, complex queries use premium models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ys9svyji7s9",
   "source": "# Test model router with different query types\nfrom src.m2_2_prompt_optimization import ModelRouter\n\nrouter = ModelRouter()\n\nprint(\"Intelligent Model Routing\")\nprint(\"=\"*60)\n\n# Test queries with varying complexity\ntest_queries = data[\"test_queries\"]\n\nfor query_data in test_queries:\n    query = query_data[\"question\"]\n    expected_complexity = query_data.get(\"complexity\", \"unknown\")\n    \n    # Analyze and route\n    decision = router.select_model(query, context=original_context[:500])\n    \n    print(f\"\\nQuery: {query[:60]}...\")\n    print(f\"  Expected complexity: {expected_complexity}\")\n    print(f\"  Complexity score: {decision['complexity_score']}\")\n    print(f\"  Selected model: {decision['model']}\")\n    print(f\"  Tier: {decision['tier']}\")\n    print(f\"  Reason: {decision['reason']}\")\n    if decision.get('complexity_factors'):\n        print(f\"  Factors: {list(decision['complexity_factors'].keys())}\")\n\n# Expected: Shows routing decisions for simple vs complex queries",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "sf2spvm5z5s",
   "source": "# Cost implications of model routing\nprint(\"\\nModel Routing Cost Analysis\")\nprint(\"=\"*60)\n\n# Simulate routing distribution\nsimple_queries_pct = 70  # 70% simple queries\ncomplex_queries_pct = 30  # 30% complex queries\n\ntotal_queries = 10_000\n\n# Cost per query by model (rough estimates)\ncost_fast = 0.0003  # gpt-3.5-turbo\ncost_premium = 0.0020  # gpt-4o\n\n# Scenario 1: All queries to premium model\nall_premium_cost = total_queries * cost_premium * 30  # monthly\n\n# Scenario 2: Smart routing\nsimple_cost = (total_queries * simple_queries_pct / 100) * cost_fast * 30\ncomplex_cost = (total_queries * complex_queries_pct / 100) * cost_premium * 30\nsmart_routing_cost = simple_cost + complex_cost\n\nsavings = all_premium_cost - smart_routing_cost\nsavings_pct = (savings / all_premium_cost * 100)\n\nprint(f\"\\nScenario 1: All queries ‚Üí Premium model\")\nprint(f\"  Monthly cost: ${all_premium_cost:.2f}\")\n\nprint(f\"\\nScenario 2: Smart routing ({simple_queries_pct}% simple, {complex_queries_pct}% complex)\")\nprint(f\"  Simple queries: ${simple_cost:.2f}/month\")\nprint(f\"  Complex queries: ${complex_cost:.2f}/month\")\nprint(f\"  Total: ${smart_routing_cost:.2f}/month\")\n\nprint(f\"\\nüí∞ Savings: ${savings:.2f}/month ({savings_pct:.1f}% reduction)\")\nprint(\"\\nüí° Routing matches complexity to model tier\")\n\n# Expected: Shows significant cost savings from routing",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "p1z6wn918wp",
   "source": "## 5. Prompt Testing Framework (A/B)\n\nRun A/B tests to compare prompt variants scientifically. Measure tokens, cost, and latency for each template.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9gzdeo48ph",
   "source": "# Set up prompt tester (will auto-detect if API key available)\nfrom src.m2_2_prompt_optimization import PromptTester\n\n# Initialize client if API key available\nopenai_client = None\nif HAS_API_KEY:\n    try:\n        from openai import OpenAI\n        openai_client = OpenAI()\n        print(\"‚úì OpenAI client initialized - will run LIVE tests\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not initialize client: {e}\")\n        print(\"‚ö†Ô∏è Skipping (no keys) - Running in DRY RUN mode\")\nelse:\n    print(\"‚ö†Ô∏è Skipping (no keys) - running in DRY RUN mode (estimates only)\")\n\n# Create tester\ntester = PromptTester(\n    openai_client=openai_client,\n    model=\"gpt-3.5-turbo\",\n    dry_run=(not HAS_API_KEY)\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PromptTester initialized\")\nprint(f\"Mode: {'LIVE API calls' if HAS_API_KEY else 'DRY RUN (estimates)'}\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1xrqh4wuqy5i",
   "source": "# Compare 3 prompt templates\ntemplates_to_test = [\n    RAGPromptLibrary.BASIC_RAG,\n    RAGPromptLibrary.CONCISE_RAG,\n    RAGPromptLibrary.STRUCTURED_RAG,\n]\n\n# Use first 3 test queries\ntest_cases = data[\"test_queries\"][:3]\n\nprint(\"\\nRunning A/B comparison...\")\nprint(f\"Testing {len(templates_to_test)} templates on {len(test_cases)} queries\")\nprint()\n\n# Run comparison\nresults = tester.compare_templates(\n    templates_to_test,\n    test_cases,\n    data[\"documents\"]\n)\n\n# Expected: Table comparing tokens, cost, and latency\n# In dry run: estimates only\n# With API key: actual measurements",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "oxa7ye8rh6a",
   "source": "## 6. Cost & Latency Projections\n\nProject monthly costs at different scales and see the impact of optimization decisions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cpmfvlonuai",
   "source": "# Use results from A/B testing to project costs\nimport pandas as pd\n\nprint(\"Monthly Cost Projections\")\nprint(\"=\"*60)\n\n# Different scale scenarios\nscales = [\n    (\"Startup\", 100),\n    (\"Growth\", 1_000),\n    (\"Production\", 10_000),\n    (\"Enterprise\", 100_000),\n]\n\n# Use the results from our comparison\nif results:\n    best_template = results[0]  # Cheapest\n    baseline_template = results[-1]  # Most expensive\n    \n    print(f\"\\nComparing:\")\n    print(f\"  Baseline: {baseline_template.template_name}\")\n    print(f\"    ${baseline_template.avg_cost_per_query:.6f}/query\")\n    print(f\"  Optimized: {best_template.template_name}\")\n    print(f\"    ${best_template.avg_cost_per_query:.6f}/query\")\n    print()\n    \n    projection_data = []\n    \n    for scale_name, queries_per_day in scales:\n        baseline_monthly = baseline_template.avg_cost_per_query * queries_per_day * 30\n        optimized_monthly = best_template.avg_cost_per_query * queries_per_day * 30\n        savings = baseline_monthly - optimized_monthly\n        savings_pct = (savings / baseline_monthly * 100) if baseline_monthly > 0 else 0\n        \n        projection_data.append({\n            \"Scale\": scale_name,\n            \"Queries/Day\": f\"{queries_per_day:,}\",\n            \"Baseline\": f\"${baseline_monthly:.2f}\",\n            \"Optimized\": f\"${optimized_monthly:.2f}\",\n            \"Savings\": f\"${savings:.2f}\",\n            \"Savings %\": f\"{savings_pct:.1f}%\"\n        })\n    \n    df = pd.DataFrame(projection_data)\n    print(df.to_string(index=False))\n    \n    print(\"\\nüí° Savings scale linearly with query volume\")\n    print(\"‚ö†Ô∏è  But implementation overhead is fixed ~8 hours\")\n\n# Expected: Table showing costs at different scales",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vyh59omejmp",
   "source": "# ROI calculation\nprint(\"\\nROI Analysis\")\nprint(\"=\"*60)\n\nimplementation_hours = 8\nhourly_rate = 100  # Developer hourly rate\nimplementation_cost = implementation_hours * hourly_rate\n\nprint(f\"\\nImplementation cost: ${implementation_cost} ({implementation_hours} hours @ ${hourly_rate}/hr)\")\nprint(\"\\nBreak-even analysis:\")\n\nif results:\n    best = results[0]\n    baseline = results[-1]\n    savings_per_query = baseline.avg_cost_per_query - best.avg_cost_per_query\n    \n    for scale_name, queries_per_day in scales:\n        daily_savings = savings_per_query * queries_per_day\n        monthly_savings = daily_savings * 30\n        \n        if monthly_savings > 0:\n            months_to_breakeven = implementation_cost / monthly_savings\n            print(f\"\\n{scale_name} ({queries_per_day:,} q/day):\")\n            print(f\"  Monthly savings: ${monthly_savings:.2f}\")\n            print(f\"  Break-even: {months_to_breakeven:.1f} months\")\n            \n            if months_to_breakeven < 1:\n                print(f\"  ‚úì ROI: EXCELLENT - pays for itself in <1 month\")\n            elif months_to_breakeven < 3:\n                print(f\"  ‚úì ROI: GOOD - pays for itself in {months_to_breakeven:.0f} months\")\n            elif months_to_breakeven < 12:\n                print(f\"  ‚ö†Ô∏è  ROI: MARGINAL - takes {months_to_breakeven:.0f} months\")\n            else:\n                print(f\"  ‚ùå ROI: POOR - takes {months_to_breakeven:.0f} months\")\n\nprint(\"\\nüí° Optimization is worth it at 1K+ queries/day\")\n\n# Expected: ROI analysis showing when optimization makes sense",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nm88b8icc8m",
   "source": "## 7. Common Failures & When NOT to Optimize\n\nLearn what breaks with prompt optimization and when to avoid it entirely.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ey54ptmoii",
   "source": "### Common Failure Modes\n\n**Failure #1: Token Limit Exceeded Despite Optimization**\n- **Cause:** Forgot to account for prompt overhead + safety margin\n- **Fix:** Reserve tokens: `actual_limit = model_context - prompt_overhead - safety_margin`\n\n**Failure #2: Model Router Selects Wrong Tier**\n- **Cause:** Complexity scoring over-weights query length\n- **Fix:** Combine length with reasoning keyword detection; manual override for known patterns\n\n**Failure #3: Aggressive Truncation Loses Critical Context**\n- **Cause:** Cutting mid-sentence, removing exceptions/caveats\n- **Fix:** Truncate at sentence boundaries, add `[truncated]` indicators\n\n**Failure #4: Cache Invalidation Causing Cost Spikes**\n- **Cause:** Cache keys include prompt hash; template updates invalidate all caches\n- **Fix:** Use semantic versioning (v1, v2) not exact hashes; implement fallback to previous versions\n\n**Failure #5: JSON Output Format Breaking**\n- **Cause:** Model ignores \"return JSON only\" instruction\n- **Fix:** Use `response_format={\"type\": \"json_object\"}`, lower temperature to 0.0, validate + retry",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "swlf1k4pu9l",
   "source": "### When NOT to Use Prompt Optimization\n\n**‚ùå Don't optimize when:**\n\n1. **Response Quality is Non-Negotiable**\n   - Medical advice, legal analysis, financial recommendations\n   - Use instead: Best model (GPT-4) with full prompts + human review\n   - Example: Medical diagnosis - patient safety >>> cost savings\n\n2. **Query Volume Too Low (<100 queries/day)**\n   - Implementation overhead (8-12 hours) exceeds savings\n   - At 100 q/day with $0.002/query = $6/month cost\n   - Use instead: Keep prompts simple and clear\n\n3. **Query Diversity Extremely High (>90% unique)**\n   - Caching ineffective, uniform optimization difficult\n   - Use instead: Focus on infrastructure or consider fine-tuning\n   - Example: Research assistant with novel academic queries\n\n**üö© Warning Signs:**\n- Users report \"answers feel rushed or incomplete\" ‚Üí too aggressive\n- Cache hit rate <10% ‚Üí query diversity too high\n- Costs still >$500/month after optimization ‚Üí consider fine-tuning\n- Quality metrics declining ‚Üí token cuts removing necessary context\n- More time tuning than saving ‚Üí volume too low",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w18a6wbfd3b",
   "source": "# Decision framework\nprint(\"DECISION CARD: Should You Optimize Prompts?\")\nprint(\"=\"*60)\n\n# User should fill these in for their use case\nyour_queries_per_day = 1000  # Change this\nyour_monthly_cost = 300      # Change this\nquality_critical = False     # Change this\n\nprint(f\"\\nYour situation:\")\nprint(f\"  Queries per day: {your_queries_per_day:,}\")\nprint(f\"  Monthly LLM cost: ${your_monthly_cost:.2f}\")\nprint(f\"  Quality critical: {'Yes' if quality_critical else 'No'}\")\nprint()\n\n# Decision logic\nrecommendation = None\n\nif quality_critical:\n    recommendation = \"‚ùå DON'T OPTIMIZE - Quality is non-negotiable\"\nelif your_queries_per_day < 100:\n    recommendation = \"‚ùå DON'T OPTIMIZE - Volume too low, overhead exceeds savings\"\nelif your_monthly_cost < 50:\n    recommendation = \"‚ùå DON'T OPTIMIZE - Cost too low to justify effort\"\nelif your_queries_per_day >= 10000:\n    recommendation = \"‚úì‚úì STRONGLY RECOMMEND - High volume, significant savings potential\"\nelif your_queries_per_day >= 1000:\n    recommendation = \"‚úì RECOMMEND - Good volume, ROI positive\"\nelse:\n    recommendation = \"‚ö†Ô∏è  MARGINAL - Consider if growth expected\"\n\nprint(f\"Recommendation: {recommendation}\")\nprint()\n\n# Projected savings\nif your_monthly_cost > 0 and not quality_critical:\n    estimated_savings = your_monthly_cost * 0.35  # Conservative 35%\n    print(f\"Estimated monthly savings: ${estimated_savings:.2f} (35% reduction)\")\n    \n    implementation_cost = 800  # 8 hours @ $100/hr\n    months_to_breakeven = implementation_cost / estimated_savings if estimated_savings > 0 else 999\n    \n    print(f\"Break-even: {months_to_breakeven:.1f} months\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üí° Use this decision card for your specific use case\")\n\n# Expected: Personalized recommendation based on inputs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6zjeivb174h",
   "source": "## Summary & Next Steps\n\n**What we learned:**\n1. ‚úì RAG-specific prompt templates can reduce tokens by 30-60%\n2. ‚úì Model routing matches complexity to appropriate tier\n3. ‚úì Token optimization requires sentence-boundary truncation\n4. ‚úì A/B testing framework measures real impact\n5. ‚úì ROI is positive at 1K+ queries/day\n6. ‚úì Common failures are predictable and fixable\n7. ‚úì Optimization is NOT for everyone (quality-critical, low volume, high diversity)\n\n**Key Takeaway:**\nPrompt optimization is a **tool**, not a mandate. Let economics guide your engineering decisions.\n\n**Action Items:**\n1. [ ] Calculate your current token usage and costs\n2. [ ] Test 2-3 prompt templates with your real data\n3. [ ] Measure quality impact (not just cost)\n4. [ ] Use decision card to determine if optimization is worth it\n5. [ ] If proceeding: Implement monitoring before optimizing\n\n**When in doubt:** Start conservative, measure everything, optimize incrementally.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}