{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ul5z5qz3ox",
   "source": "---\n\n## Summary\n\nYou now have a **production-grade monitoring system** for RAG applications:\n\n\u2705 **Metrics collection** with Prometheus client library  \n\u2705 **Structured logging** for cloud environments  \n\u2705 **Automatic instrumentation** via decorators  \n\u2705 **Visualization** with Grafana dashboards  \n\u2705 **Alert templates** for critical issues  \n\n**Next steps:**\n1. Run `docker compose -f docker-compose.monitoring.yml up -d`\n2. Start your RAG service with metrics enabled\n3. Import `grafana_dash.json` into Grafana\n4. Set up 3-5 critical alerts\n5. Monitor for 1 week and adjust thresholds\n\n**Remember:**\n- Start simple (3-5 metrics, 3-5 alerts)\n- Tune thresholds based on real traffic\n- Consider managed alternatives if infrastructure burden is too high\n- Monitor your monitoring (storage, query performance)\n\n**Total setup time:** 8-12 hours initial + 2-4 hours/month maintenance  \n**Value:** Real-time visibility into performance, costs, and quality \ud83d\ude80",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "3xhxhpdujp4",
   "source": "### Common Failure Modes to Watch For\n\n**1. Metric Cardinality Explosion**\n- **Problem:** Using user IDs or query text as labels creates millions of unique metrics\n- **Solution:** Use bounded labels (endpoint, model, error_type) and aggregate high-cardinality data in logs\n\n**2. Dashboard Query Timeouts**\n- **Problem:** Complex PromQL queries on large time ranges\n- **Solution:** Use recording rules to pre-compute expensive aggregations\n\n**3. Memory Leaks from Unbounded Labels**\n- **Problem:** Labels with infinite possible values (timestamps, UUIDs)\n- **Solution:** Validate label cardinality < 1000 per metric\n\n**4. Alert Fatigue**\n- **Problem:** Too many alerts or poorly calibrated thresholds\n- **Solution:** Start with 3-5 critical alerts, tune thresholds over 1 week of real traffic\n\n**5. Metric Gaps After Deployment**\n- **Problem:** Metrics server restarts lose in-memory state\n- **Solution:** Use Prometheus federation or remote storage for persistence",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "10skhyuz8moe",
   "source": "### Example Alert Rules\n\nHere are 5 critical alerts to set up (via Prometheus Alertmanager or Grafana Alerts):\n\n```yaml\n# Alert 1: High Latency\nalert: HighQueryLatency\nexpr: histogram_quantile(0.95, rate(rag_query_latency_seconds_bucket[5m])) > 2\nfor: 5m\nannotations:\n  summary: \"p95 latency exceeds 2 seconds\"\n\n# Alert 2: High Error Rate\nalert: HighErrorRate\nexpr: rate(rag_requests_total{status=\"error\"}[5m]) / rate(rag_requests_total[5m]) > 0.05\nfor: 2m\nannotations:\n  summary: \"Error rate above 5%\"\n\n# Alert 3: Low Cache Hit Rate\nalert: LowCacheHitRate\nexpr: rag_cache_hit_rate < 0.30\nfor: 10m\nannotations:\n  summary: \"Cache hit rate below 30%\"\n\n# Alert 4: High Cost Burn\nalert: HighCostBurn\nexpr: sum(rate(rag_total_cost_usd[1h])) * 24 > 100\nfor: 15m\nannotations:\n  summary: \"Daily spend projected to exceed $100\"\n\n# Alert 5: Rate Limit Warning\nalert: RateLimitWarning\nexpr: rag_rate_limit_remaining < 100\nfor: 1m\nannotations:\n  summary: \"Less than 100 API calls remaining\"\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "banydg4h89b",
   "source": "### Importing the Dashboard\n\n**Steps in Grafana:**\n\n1. Navigate to http://localhost:3000\n2. Log in (admin/admin)\n3. Click **+** \u2192 **Import dashboard**\n4. Upload `grafana_dash.json`\n5. Select Prometheus as the data source\n6. Click **Import**\n\n**Result:** You'll see all 10 panels populated with real-time data from your metrics endpoint!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rcuyd1im36l",
   "source": "# Dashboard panels we've configured\ndashboard_panels = {\n    'Panel': [\n        'Query Latency (p50/p95/p99)',\n        'Request Rate & Error Rate',\n        'Token Usage (Input vs Output)',\n        'Cost per Query & Total Spend',\n        'Cache Hit Rate %',\n        'Error Rate %',\n        'Active Requests',\n        'Rate Limit Remaining',\n        'Response Relevance Score',\n        'Error Breakdown by Type'\n    ],\n    'Visualization': [\n        'Time series graph',\n        'Time series graph',\n        'Time series graph',\n        'Time series graph',\n        'Gauge',\n        'Stat panel',\n        'Stat panel',\n        'Gauge',\n        'Time series graph',\n        'Pie chart'\n    ],\n    'Purpose': [\n        'Track performance trends',\n        'Monitor traffic & failures',\n        'Understand token consumption',\n        'Control spending',\n        'Optimize caching strategy',\n        'Set SLA alerts',\n        'Detect traffic spikes',\n        'Prevent rate limit hits',\n        'Monitor answer quality',\n        'Debug error patterns'\n    ]\n}\n\ndf_panels = pd.DataFrame(dashboard_panels)\nprint(df_panels.to_string(index=False))\n\n# Expected: Table of 10 dashboard panels",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4my67voj0bb",
   "source": "## 7. Dashboards & Alerts (Conceptual)\n\nOnce Grafana is running, you can import `grafana_dash.json` to get a pre-built dashboard.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "sb61e21s5ki",
   "source": "### Prometheus Scrape Configuration\n\nThe `prometheus.yml` file tells Prometheus where to collect metrics:\n\n```yaml\nscrape_configs:\n  - job_name: 'rag-service'\n    static_configs:\n      - targets: ['host.docker.internal:8000']\n    scrape_interval: 10s\n```\n\n**Key settings:**\n- `targets`: Where to scrape (our metrics endpoint)\n- `scrape_interval`: How often to collect (10s = every 10 seconds)\n- `retention`: How long to store data (default 30 days)\n\n**Prometheus queries (PromQL):**\n- `rag_query_latency_seconds` \u2192 Raw histogram data\n- `histogram_quantile(0.95, ...)` \u2192 Calculate p95 latency\n- `rate(rag_requests_total[5m])` \u2192 Requests per second over 5min",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "80mgk9lngta",
   "source": "import subprocess\nimport os\n\n# Check if docker-compose file exists\ncompose_file = \"docker-compose.monitoring.yml\"\nprometheus_config = \"prometheus.yml\"\n\nif os.path.exists(compose_file) and os.path.exists(prometheus_config):\n    print(f\"\u2713 Found {compose_file}\")\n    print(f\"\u2713 Found {prometheus_config}\")\n    print(\"\\nReady to start monitoring stack!\")\n    print(\"\\nRun in terminal:\")\n    print(f\"  docker compose -f {compose_file} up -d\")\n    print(\"\\nThen visit:\")\n    print(\"  \u2022 Prometheus: http://localhost:9090\")\n    print(\"  \u2022 Grafana: http://localhost:3000 (admin/admin)\")\nelse:\n    print(\"\u26a0 Docker compose files not found\")\n    print(\"  Expected: docker-compose.monitoring.yml and prometheus.yml\")\n\n# Expected: Success message with URLs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s5iionuv4a",
   "source": "### Starting the Monitoring Stack\n\n**If Docker is installed**, run:\n\n```bash\n# Start Prometheus + Grafana\ndocker compose -f docker-compose.monitoring.yml up -d\n\n# Check status\ndocker compose -f docker-compose.monitoring.yml ps\n```\n\n**Services:**\n- \ud83d\udd0d **Prometheus**: http://localhost:9090 (metrics database)\n- \ud83d\udcca **Grafana**: http://localhost:3000 (login: admin/admin)\n\n**If Docker is NOT running:**\n- You can still use the metrics endpoint for testing\n- Cloud providers offer managed Prometheus/Grafana alternatives\n- See README.md for installation instructions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "o9hok60s4f",
   "source": "## 6. Prometheus + Grafana Setup\n\nTo visualize metrics, we need to run Prometheus (metrics storage) and Grafana (dashboards).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "r5ppsbbc8",
   "source": "### Example JSON Log Output\n\n```json\n{\n  \"timestamp\": \"2025-11-06T12:34:56.789Z\",\n  \"level\": \"INFO\",\n  \"logger\": \"__main__\",\n  \"message\": \"Response completed\",\n  \"service\": \"rag-service\",\n  \"environment\": \"development\",\n  \"event_type\": \"response\",\n  \"duration_ms\": 1250.5,\n  \"input_tokens\": 450,\n  \"output_tokens\": 180,\n  \"cost_usd\": 0.0234,\n  \"success\": true,\n  \"model\": \"gpt-4\"\n}\n```\n\n**Benefits:**\n- Cloud logging services can parse and index these automatically\n- Query by `cost_usd > 0.05` or `error_type = \"RateLimitError\"`\n- Correlate with metrics using timestamps",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dtarv0lyu7u",
   "source": "from src.m2_3_monitoring import StructuredLogger\n\n# Create a structured logger\nlogger = StructuredLogger(__name__)\n\n# Log a request\nlogger.log_request(\n    query=\"What are the latest sales figures?\",\n    user_id=\"alice\",\n    session_id=\"sess_12345\",\n    endpoint=\"/api/query\"\n)\n\n# Simulate processing and log response\nlogger.log_response(\n    duration_ms=1250.5,\n    tokens={'input': 450, 'output': 180},\n    cost=0.0234,\n    success=True,\n    model=\"gpt-4\",\n    cache_hit=False\n)\n\n# Log an error scenario\ntry:\n    raise ValueError(\"Simulated error: API rate limit exceeded\")\nexcept Exception as e:\n    logger.log_error(e, context={\n        'user_id': 'alice',\n        'endpoint': '/api/query',\n        'retry_attempt': 3\n    })\n\nprint(\"\\n\u2713 Structured logs emitted (check console output above)\")\nprint(\"  Each log is a JSON object with timestamp, level, service, and custom fields\")\n\n# Expected: 3 JSON log lines printed to console",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "j5o6vl7201",
   "source": "## 5. Structured Logging\n\nProduction systems need **structured logs** (JSON format) for:\n- Cloud log aggregation (CloudWatch, Stackdriver, Datadog)\n- Searchability by fields (user_id, error_type, cost, etc.)\n- Correlation with metrics (matching timestamps)\n\nTraditional logs: `\"User alice made query at 2023-10-15\"` \u274c  \nStructured logs: `{\"timestamp\": \"...\", \"user_id\": \"alice\", \"event\": \"query\"}` \u2705",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "jlmioej5hxe",
   "source": "### How the Decorator Works\n\nThe `@monitored_query` decorator automatically:\n\n1. \u2705 **Starts a timer** when the function is called\n2. \u2705 **Increments active_requests** gauge\n3. \u2705 **Records latency** in the histogram when complete\n4. \u2705 **Extracts token counts** from the return value\n5. \u2705 **Calculates cost** based on token usage\n6. \u2705 **Records relevance scores** if present\n7. \u2705 **Catches errors** and records them separately\n\n**No manual metric recording needed** - just return a dict with the right keys!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wtmrsrnxaba",
   "source": "from src.m2_3_monitoring import monitored_query, track_cache_operation\nimport time\nimport random\n\n@monitored_query(operation=\"rag_pipeline\", model=\"gpt-4\")\ndef simulated_rag_query(query: str, use_cache: bool = False):\n    \"\"\"\n    Simulates a RAG query with retrieval, LLM generation, and metrics.\n    \"\"\"\n    \n    # Simulate cache check\n    cache_hit = use_cache and random.random() > 0.3\n    track_cache_operation(hit=cache_hit, cache_type=\"semantic\")\n    \n    if cache_hit:\n        # Fast path - return cached result\n        time.sleep(0.05)\n        return {\n            'answer': 'Cached answer',\n            'input_tokens': 450,\n            'output_tokens': 120,\n            'relevance_score': 0.88,\n            'cached': True\n        }\n    \n    # Simulate retrieval (100-500ms)\n    retrieval_time = random.uniform(0.1, 0.5)\n    metrics.retrieval_latency.observe(retrieval_time)\n    time.sleep(retrieval_time)\n    \n    # Simulate LLM generation (1-3s)\n    llm_time = random.uniform(1.0, 3.0)\n    metrics.llm_latency.observe(llm_time)\n    time.sleep(llm_time)\n    \n    # Return realistic metrics\n    return {\n        'answer': f'Generated answer for: {query[:30]}...',\n        'input_tokens': random.randint(400, 1500),\n        'output_tokens': random.randint(100, 500),\n        'relevance_score': random.uniform(0.7, 0.95),\n        'cached': False\n    }\n\n# Run sample queries\nprint(\"Running 5 sample RAG queries...\\n\")\n\nfor i in range(5):\n    use_cache = (i % 2 == 0)  # Alternate cache usage\n    result = simulated_rag_query(\n        query=f\"What is the status of project {i}?\",\n        use_cache=use_cache\n    )\n    \n    print(f\"Query {i+1}: tokens={result['input_tokens']}+{result['output_tokens']}, \"\n          f\"relevance={result['relevance_score']:.2f}, cached={result['cached']}\")\n\nprint(\"\\n\u2713 Metrics recorded and available at /metrics endpoint\")\n\n# Expected: 5 query results with varying token counts and relevance scores",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cvm4r2lr3xf",
   "source": "## 4. Instrumenting a RAG Query\n\nLet's simulate a complete RAG pipeline and automatically record metrics using the `@monitored_query` decorator.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "uxbrkavleee",
   "source": "### What Prometheus Sees\n\nWhen Prometheus scrapes the `/metrics` endpoint, it gets text output like:\n\n```\n# HELP rag_query_latency_seconds End-to-end RAG query latency\n# TYPE rag_query_latency_seconds histogram\nrag_query_latency_seconds_bucket{le=\"0.1\",operation=\"query\"} 5\nrag_query_latency_seconds_bucket{le=\"0.5\",operation=\"query\"} 12\nrag_query_latency_seconds_bucket{le=\"1.0\",operation=\"query\"} 18\nrag_query_latency_seconds_sum{operation=\"query\"} 15.3\nrag_query_latency_seconds_count{operation=\"query\"} 20\n```\n\nThis is the **Prometheus exposition format** - a simple text protocol that Grafana queries.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kxwz2i8vqy",
   "source": "from src.m2_3_monitoring import start_metrics_server, metrics\nimport config\n\n# Start the Prometheus metrics HTTP server\n# This will serve metrics at http://localhost:8000/metrics\nport = config.METRICS_PORT\nsuccess = start_metrics_server(port)\n\nif success:\n    print(f\"\u2713 Metrics endpoint running at http://localhost:{port}/metrics\")\n    print(f\"\u2713 Prometheus will scrape this endpoint every 15s\")\nelse:\n    print(f\"\u26a0 Metrics server may already be running on port {port}\")\n\n# Expected: Success message with URL",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nfi6f2yrnv",
   "source": "## 3. Start Metrics Endpoint\n\nOur `m2_3_monitoring.py` module provides a simple `start_metrics_server()` function that exposes metrics on `/metrics`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "k5332a4z9pr",
   "source": "### Understanding Metric Types\n\n**Prometheus supports 4 metric types:**\n\n1. **Counter** - Only goes up (total requests, cumulative cost)\n2. **Gauge** - Can go up/down (active connections, cache hit rate)\n3. **Histogram** - Samples observations and buckets them (latency, token counts)\n4. **Summary** - Similar to histogram but calculates quantiles client-side (less common)\n\n**Why histograms for latency?**\n- Allows querying p50/p95/p99 percentiles\n- Better than averages (which hide outliers)\n- Example: p95 = 2s means \"95% of requests complete within 2 seconds\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h47pkz5xtam",
   "source": "# Define our metric categories\nmetrics_catalog = {\n    'Category': [\n        'Performance', 'Performance', 'Performance',\n        'Cost', 'Cost',\n        'Quality', 'Quality',\n        'System Health', 'System Health', 'System Health'\n    ],\n    'Metric': [\n        'Query Latency (p50/p95/p99)', 'Retrieval Time', 'LLM Generation Time',\n        'Cost per Query', 'Daily Spend by Model',\n        'Relevance Score', 'Context Precision',\n        'Cache Hit Rate', 'Error Rate', 'Rate Limit Headroom'\n    ],\n    'Type': [\n        'Histogram', 'Histogram', 'Histogram',\n        'Histogram', 'Counter',\n        'Histogram', 'Histogram',\n        'Gauge', 'Counter', 'Gauge'\n    ],\n    'Alert Threshold': [\n        'p95 > 2s', 'p95 > 500ms', 'p95 > 5s',\n        'p95 > $0.10', 'Daily > $100',\n        'p50 < 0.7', 'p50 < 0.6',\n        '< 30%', '> 5%', '< 100 requests'\n    ]\n}\n\ndf_metrics = pd.DataFrame(metrics_catalog)\nprint(df_metrics.to_string(index=False))\n\n# Expected: Table of 10 metrics with types and alert thresholds",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3pd086nhoyo",
   "source": "## 2. Metrics We'll Collect & Why\n\nA production RAG system needs 4 categories of metrics: **Performance**, **Cost**, **Quality**, and **System Health**.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8bpkl0v3759",
   "source": "### When Self-Hosted Monitoring is Overkill\n\n**Don't use Prometheus/Grafana if:**\n- You're processing < 500 queries/day \u2192 Use CloudWatch/Stackdriver logs\n- Your team has no DevOps experience \u2192 Use Datadog/New Relic\n- You need results in < 2 hours \u2192 Start with basic logging first\n\n**DO use it when:**\n- High query volumes justify the infrastructure cost\n- You need granular control over data retention\n- Your team can maintain the monitoring stack\n- You want industry-standard open-source tools",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qf1bnaumqst",
   "source": "import pandas as pd\n\n# Three monitoring approaches compared\napproaches = {\n    'Approach': ['Self-Hosted (Prometheus)', 'Managed APM', 'Native Cloud Logging'],\n    'Setup Time': ['8-12 hours', '30 minutes', '1 hour'],\n    'Monthly Cost': ['$50-200', '$15-31/host', '$5-20'],\n    'Storage Growth': ['1-5 GB/day', 'Vendor-managed', '~100 MB/day'],\n    'Best For': ['>1K queries/day', 'Non-technical teams', '<500 queries/day'],\n    'Learning Curve': ['Steep', 'Minimal', 'Low']\n}\n\ndf = pd.DataFrame(approaches)\nprint(df.to_string(index=False))\n\n# Expected: Table showing 3 approaches with tradeoffs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xws6kjrpo3",
   "source": "## 1. Reality Check & When This is Overkill\n\nBefore diving into Prometheus and Grafana, let's understand the **trade-offs** of self-hosted monitoring.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "7vo3algerm",
   "source": "# M2.3 \u2014 Production Monitoring Dashboard\n\n**Goal:** Implement production-grade monitoring with Prometheus & Grafana for RAG systems.\n\n**Topics:**\n- Performance metrics (latency, tokens, costs)\n- Structured logging for cloud environments\n- Dashboard visualization and alerting strategies\n- When to use managed alternatives\n\n**Duration:** ~40 minutes\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}