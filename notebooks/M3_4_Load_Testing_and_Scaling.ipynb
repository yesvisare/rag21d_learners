{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3.4 — Load Testing & Scaling for RAG Systems\n",
    "\n",
    "**Module**: M3 - Production RAG  \n",
    "**Focus**: Locust, bottlenecks, horizontal/vertical scaling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Objectives & Reality Check\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Implement** load testing with Locust for RAG systems\n",
    "2. **Execute** 5 test types: smoke, load, stress, spike, and soak tests\n",
    "3. **Interpret** performance metrics: p50, p95, p99 latencies, throughput, error rates\n",
    "4. **Identify** bottlenecks: application code, external services, infrastructure\n",
    "5. **Apply** scaling strategies: horizontal vs vertical, caching, batching, auto-scaling\n",
    "6. **Decide** when to load test and when to skip it\n",
    "\n",
    "### Why Load Testing Matters\n",
    "\n",
    "Load testing answers critical questions:\n",
    "\n",
    "- **Capacity**: How many concurrent users can your system handle?\n",
    "- **Latency**: What's the user experience under load? (p95, p99 reveal worst cases)\n",
    "- **Reliability**: At what point does the system start failing?\n",
    "- **Bottlenecks**: Is it the database? External API? Application code?\n",
    "\n",
    "### The Reality Check: When NOT to Load Test\n",
    "\n",
    "**Skip load testing when:**\n",
    "\n",
    "1. **Small scale**: 50 daily users with no growth trajectory  \n",
    "   *Cost*: 8-12 hours setup + 2-4 hours per sprint maintenance  \n",
    "   *Benefit*: Minimal for non-critical systems\n",
    "\n",
    "2. **Staging ≠ Production**: Different environments yield unreliable predictions  \n",
    "   *Example*: Staging has 1GB test data, production has 100GB  \n",
    "   *Result*: Load test shows 2s latency, production experiences 20s\n",
    "\n",
    "3. **Early-stage uncertainty**: User patterns unknown, architecture may pivot  \n",
    "   *Better approach*: Optimize obvious bottlenecks first (N+1 queries, missing indexes)\n",
    "\n",
    "**Use load testing when:**\n",
    "\n",
    "1. **Growth trajectory**: Approaching capacity limits (e.g., 70% CPU sustained)\n",
    "2. **SLA requirements**: Performance guarantees in contracts (e.g., p99 <2s)\n",
    "3. **Infrastructure decisions**: Justifying $500/month upgrade with data\n",
    "4. **Pre-production validation**: Confirming system meets capacity targets\n",
    "\n",
    "### Key Metrics Explained\n",
    "\n",
    "| Metric | Definition | Target (typical) | Why It Matters |\n",
    "|--------|------------|------------------|----------------|\n",
    "| **Throughput** | Requests per second (RPS) | 20-100 RPS | System capacity |\n",
    "| **p50 Latency** | Median response time | <500ms | Typical user experience |\n",
    "| **p95 Latency** | 95th percentile response | <2s | Most users' experience |\n",
    "| **p99 Latency** | 99th percentile response | <5s | Worst-case user experience |\n",
    "| **Error Rate** | % of failed requests | <0.1% | System reliability |\n",
    "| **Concurrency** | Simultaneous users | 50-500 | Peak load capacity |\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "**Time investment**:  \n",
    "- Initial setup: 8-12 hours (locustfile, infrastructure, baseline tests)  \n",
    "- Per-sprint maintenance: 2-4 hours (update tests, analyze results)  \n",
    "\n",
    "**Value delivered**:  \n",
    "- Prevent outages during traffic spikes  \n",
    "- Data-driven infrastructure decisions (avoid over-provisioning)  \n",
    "- Identify bottlenecks before production exposure  \n",
    "- Confidence in scaling strategy  \n",
    "\n",
    "**Break-even point**: Systems expecting >500 concurrent users or revenue-critical applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment setup\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if Locust is available\n",
    "try:\n",
    "    import locust\n",
    "    print(f\"Locust version: {locust.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  Locust not installed. Run: pip install locust==2.31.6\")\n",
    "\n",
    "# Expected: Python 3.8+ and Locust 2.31.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reality Check Exercise\n",
    "\n",
    "**Question**: Should you load test in these scenarios?\n",
    "\n",
    "1. **Scenario A**: Internal tool used by 10 employees, no external traffic  \n",
    "   **Answer**: ❌ No - overhead not justified for small, stable usage\n",
    "\n",
    "2. **Scenario B**: Public API with 1000 daily users, growing 20% monthly  \n",
    "   **Answer**: ✅ Yes - growth trajectory demands capacity planning\n",
    "\n",
    "3. **Scenario C**: E-commerce site with SLA: p95 <1s, 99.9% uptime  \n",
    "   **Answer**: ✅ Yes - contractual obligations require validation\n",
    "\n",
    "4. **Scenario D**: MVP with 50 beta users, architecture may change  \n",
    "   **Answer**: ❌ No - premature optimization, focus on product-market fit\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Section 2 covers Locust setup and the 5 test types (smoke, load, stress, spike, soak)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Locust Setup & Test Types\n\n### Installation & Project Structure\n\n**Install Locust**:\n```bash\npip install locust==2.31.6 python-dotenv==1.0.1\n```\n\n**Project structure**:\n```\nproject/\n├── locustfile.py          # Test scenarios\n├── .env                   # Configuration (TARGET_URL)\n├── .env.example           # Template for .env\n├── requirements.txt       # Dependencies\n└── results/               # Test output (CSV, HTML reports)\n```\n\n### The 5 Test Types\n\n| Test Type | Purpose | Users | Duration | When to Use |\n|-----------|---------|-------|----------|-------------|\n| **Smoke** | Health check | 10 | 2 min | Before each deployment |\n| **Load** | Normal capacity | 100 | 10 min | Weekly baseline |\n| **Stress** | Breaking point | 1000+ | 15 min | Quarterly capacity planning |\n| **Spike** | Traffic surge | 500 instant | 5 min | Before marketing campaigns |\n| **Soak** | Memory leaks | 50 | 4 hours | Monthly stability check |\n\n### Test Type Details\n\n#### 1. Smoke Test\n**Goal**: Verify system works under minimal load  \n**Scenario**: 10 users, 2 minutes  \n**Success criteria**: 0% error rate, p95 <3s\n\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 10 --spawn-rate 2 --run-time 2m --headless\n```\n\n**Use case**: Run after every deployment to catch basic breakage\n\n---\n\n#### 2. Load Test\n**Goal**: Validate system handles expected traffic  \n**Scenario**: 100 users, 10 minutes  \n**Success criteria**: Error rate <1%, p95 <2s\n\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 100 --spawn-rate 10 --run-time 10m --headless\n```\n\n**Use case**: Weekly regression test, establish performance baseline\n\n---\n\n#### 3. Stress Test\n**Goal**: Find breaking point  \n**Scenario**: Gradually increase to 1000+ users  \n**Success criteria**: Identify at what user count error rate exceeds 5%\n\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 1000 --spawn-rate 50 --run-time 15m --headless\n```\n\n**Use case**: Capacity planning, infrastructure sizing\n\n---\n\n#### 4. Spike Test\n**Goal**: Simulate sudden traffic surge (e.g., HackerNews front page)  \n**Scenario**: Jump from 10 to 500 users instantly  \n**Success criteria**: System recovers without manual intervention\n\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 500 --spawn-rate 500 --run-time 5m --headless\n```\n\n**Use case**: Before product launches, marketing campaigns\n\n---\n\n#### 5. Soak Test\n**Goal**: Detect memory leaks, resource exhaustion  \n**Scenario**: 50 users sustained for 4 hours  \n**Success criteria**: Stable memory, no degradation over time\n\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 50 --spawn-rate 5 --run-time 4h --headless\n```\n\n**Use case**: Before major releases, after architectural changes\n\n---\n\n### Locust File Anatomy\n\nSee `locustfile.py` in this repository for full implementation. Key components:\n\n1. **HttpUser class**: Defines simulated user behavior\n2. **@task decorators**: Weighted task distribution (10:3:1 ratio)\n3. **wait_time**: Realistic thinking time between requests (1-3s)\n4. **catch_response=True**: Granular success/failure handling\n5. **Event listeners**: Track test start/stop with statistics\n\n**Task weights** simulate realistic traffic:\n- `/query` endpoint: weight=10 (most frequent)\n- `/retrieval` endpoint: weight=3\n- `/health` endpoint: weight=1\n\n**Why wait_time matters**: Without it, Locust sends requests non-stop (unrealistic). Real users think, scroll, type between actions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inspect locustfile.py structure\nwith open('locustfile.py', 'r') as f:\n    lines = f.readlines()\n    print(f\"Total lines: {len(lines)}\")\n    print(\"\\nKey components found:\")\n    for i, line in enumerate(lines, 1):\n        if 'class RAGUser' in line:\n            print(f\"  Line {i}: RAGUser class definition\")\n        elif '@task' in line:\n            print(f\"  Line {i}: Task decorator (weighted)\")\n        elif 'wait_time' in line:\n            print(f\"  Line {i}: Wait time configuration\")\n\n# Expected: \n# - RAGUser class\n# - Multiple @task decorators with weights\n# - wait_time = between(1, 3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Running Tests & Reading p50/p95/p99\n\n### Starting Locust\n\n**Option 1: Web UI** (recommended for exploration)\n```bash\nlocust -f locustfile.py --host=http://localhost:8000\n```\nThen open `http://localhost:8089` in browser and configure:\n- Number of users\n- Spawn rate\n- Host (if not set via --host)\n\n**Option 2: Headless CLI** (recommended for CI/CD)\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 100 --spawn-rate 10 --run-time 10m --headless\n```\n\n### Understanding Locust Output\n\n**CLI output example**:\n```\nType     Name              # reqs   # fails  Avg    Min    Max    Median p95    p99   req/s failures/s\nPOST     /query            1523     12       856    234    3421   780    1850   2340  12.3  0.1\nGET      /retrieval        458      2        234    112    890    210    520    670   3.7   0.0\nGET      /health           152      0        45     23     120    42     89     110   1.2   0.0\n```\n\n**Key columns**:\n- **Avg**: Mean response time (ms) - **less important** than percentiles\n- **Median (p50)**: Half of requests faster, half slower - **typical user**\n- **p95**: 95% of requests faster - **most users' experience**\n- **p99**: 99% of requests faster - **worst-case user experience**\n- **req/s**: Throughput (requests per second)\n- **failures/s**: Error rate\n\n### Interpreting Percentiles\n\n**Why p95/p99 matter more than average**:\n\n| Scenario | Avg | p50 | p95 | p99 | Interpretation |\n|----------|-----|-----|-----|-----|----------------|\n| **Healthy** | 500ms | 450ms | 800ms | 1200ms | Consistent performance |\n| **Outliers** | 600ms | 450ms | 3500ms | 8000ms | Some users have terrible experience |\n| **Degrading** | 2000ms | 1800ms | 5000ms | 10000ms | System struggling under load |\n\n**Example**: Average 500ms looks good, but p99=8s means 1% of users wait 8 seconds!\n\n### Success Criteria by Test Type\n\n| Test Type | p50 Target | p95 Target | p99 Target | Error Rate |\n|-----------|------------|------------|------------|------------|\n| **Smoke** | <500ms | <1s | <2s | 0% |\n| **Load** | <1s | <2s | <3s | <1% |\n| **Stress** | N/A | N/A | N/A | Find breaking point |\n| **Spike** | <2s | <5s | <10s | <5% during spike |\n| **Soak** | Stable (no increase) | Stable | Stable | <0.5% |\n\n### Common Patterns in Results\n\n**Pattern 1: Rate limiting**\n```\n# reqs   # fails   p95      p99\n2000     0         850ms    1200ms   ← Below rate limit\n3000     500       920ms    8500ms   ← 429 errors start\n```\n**Diagnosis**: External API rate limit hit (e.g., OpenAI 500 RPM)\n\n**Pattern 2: Connection pool exhaustion**\n```\nTime    # reqs   # fails   p95      Error message\n0-5m    1000     0         800ms    -\n5-10m   1500     300       5000ms   \"QueuePool limit reached\"\n```\n**Diagnosis**: Database connection pool too small\n\n**Pattern 3: Memory leak**\n```\nTime     # reqs   p95      Memory\n0-1h     5000     800ms    1.2GB\n1-2h     5000     950ms    1.8GB\n2-3h     5000     1800ms   2.5GB\n3-4h     5000     4500ms   3.2GB ← Degradation\n```\n**Diagnosis**: Memory leak causes GC thrashing, slows responses\n\n### Exporting Results\n\n**Generate CSV for analysis**:\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 100 --spawn-rate 10 --run-time 10m --headless \\\n  --csv=results/load_test\n```\n\n**Output files**:\n- `results/load_test_stats.csv` - Aggregated statistics\n- `results/load_test_stats_history.csv` - Time-series data\n- `results/load_test_failures.csv` - Error details\n\n**Generate HTML report**:\n```bash\nlocust -f locustfile.py --host=http://localhost:8000 \\\n  --users 100 --spawn-rate 10 --run-time 10m --headless \\\n  --html=results/report.html\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simulate parsing Locust CSV results\nimport csv\nfrom io import StringIO\n\n# Example CSV data (simulated)\ncsv_data = \"\"\"Type,Name,Request Count,Failure Count,Median Response Time,Average Response Time,Min Response Time,Max Response Time,Average Content Size,Requests/s,Failures/s,50%,66%,75%,80%,90%,95%,98%,99%,99.9%,99.99%,100%\nPOST,/query,1523,12,780,856,234,3421,1250,12.3,0.1,780,890,1100,1250,1600,1850,2100,2340,3200,3400,3421\nGET,/retrieval,458,2,210,234,112,890,450,3.7,0.0,210,230,250,270,380,520,620,670,850,880,890\nGET,/health,152,0,42,45,23,120,50,1.2,0.0,42,45,48,50,65,89,100,110,118,120,120\n\"\"\"\n\n# Parse CSV\nreader = csv.DictReader(StringIO(csv_data))\nfor row in reader:\n    endpoint = row['Name']\n    p50 = int(row['50%'])\n    p95 = int(row['95%'])\n    p99 = int(row['99%'])\n    rps = float(row['Requests/s'])\n    error_rate = (int(row['Failure Count']) / int(row['Request Count'])) * 100\n    \n    print(f\"{endpoint:15} | p50: {p50:4}ms | p95: {p95:4}ms | p99: {p99:4}ms | RPS: {rps:4.1f} | Errors: {error_rate:.1f}%\")\n\n# Expected:\n# /query         | p50:  780ms | p95: 1850ms | p99: 2340ms | RPS: 12.3 | Errors: 0.8%\n# /retrieval     | p50:  210ms | p95:  520ms | p99:  670ms | RPS:  3.7 | Errors: 0.4%\n# /health        | p50:   42ms | p95:   89ms | p99:  110ms | RPS:  1.2 | Errors: 0.0%",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Finding Bottlenecks (code, external, infra)\n\n### The 3 Bottleneck Categories\n\nLoad tests reveal where your system breaks. Bottlenecks fall into three categories:\n\n1. **Application Code**: Inefficient algorithms, blocking operations\n2. **External Services**: OpenAI rate limits, slow database queries\n3. **Infrastructure**: CPU, memory, or network constraints\n\n### Investigation Workflow\n\n```\nLoad Test → Identify symptom → Correlate metrics → Diagnose root cause → Apply fix\n```\n\n**Step 1: Run load test and capture symptoms**  \n- High p99 latency?\n- Error rate spike?\n- Throughput plateau?\n\n**Step 2: Correlate with infrastructure metrics**  \n- CPU usage during test\n- Memory utilization\n- Network I/O\n- Database connection pool\n\n**Step 3: Analyze logs for patterns**  \n- What errors occur at breaking point?\n- Are there specific requests causing slowdown?\n\n---\n\n### Bottleneck #1: Application Code\n\n**Symptoms**:\n- High CPU usage (>80%) with low concurrency\n- Latency increases linearly with users\n- No external API errors\n\n**Common culprits**:\n\n#### A. N+1 Query Problem\n```python\n# BAD: 1 query + N queries (N+1 problem)\nusers = db.query(User).all()  # 1 query\nfor user in users:\n    user.profile = db.query(Profile).filter_by(user_id=user.id).first()  # N queries\n\n# GOOD: 2 queries total (eager loading)\nusers = db.query(User).options(joinedload(User.profile)).all()\n```\n\n**Impact**: 10x reduction in database round trips\n\n#### B. Blocking Operations\n```python\n# BAD: Synchronous blocking (ties up worker thread)\ndef process_query(query):\n    result = openai.embed(query)  # Blocks for 200-500ms\n    return result\n\n# GOOD: Async non-blocking\nasync def process_query(query):\n    result = await openai.embed(query)  # Other requests processed while waiting\n    return result\n```\n\n**Impact**: 5-10x increase in concurrency capacity\n\n#### C. Inefficient Algorithms\n```python\n# BAD: O(n²) comparison\ndef find_duplicates(documents):\n    duplicates = []\n    for i, doc1 in enumerate(documents):\n        for doc2 in documents[i+1:]:\n            if similarity(doc1, doc2) > 0.9:\n                duplicates.append((doc1, doc2))\n\n# GOOD: O(n log n) with indexing\ndef find_duplicates(documents):\n    # Use locality-sensitive hashing or vector index\n    index = build_similarity_index(documents)\n    return index.find_near_duplicates(threshold=0.9)\n```\n\n---\n\n### Bottleneck #2: External Services\n\n**Symptoms**:\n- 429 errors (rate limiting)\n- High latency despite low CPU\n- Timeouts or connection errors\n\n**Common culprits**:\n\n#### A. OpenAI Rate Limits\n```\nError: Rate limit reached for gpt-4 in organization org-XXX\nLimit: 500 requests per minute (RPM)\n```\n\n**Solutions**:\n1. **Cache embeddings**: Eliminate redundant API calls\n2. **Batch requests**: Embed multiple texts in one API call\n3. **Retry with backoff**: Handle transient rate limits\n4. **Upgrade tier**: Purchase higher rate limits\n\n**Example caching impact**:\n- Before: 1000 queries → 1000 OpenAI calls → $0.20\n- After: 1000 queries → 200 OpenAI calls (80% cache hit) → $0.04\n\n#### B. Database Query Slowness\n```python\n# Monitor slow queries\nimport time\n\ndef log_slow_queries(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        duration = time.time() - start\n        if duration > 1.0:  # Log queries >1s\n            print(f\"SLOW QUERY: {func.__name__} took {duration:.2f}s\")\n        return result\n    return wrapper\n\n@log_slow_queries\ndef search_documents(query_embedding):\n    return db.query(Document).filter(...).all()\n```\n\n**Fixes**:\n- Add indexes on frequently queried columns\n- Use vector database (FAISS, Pinecone) instead of brute-force search\n- Implement query result caching\n\n#### C. Third-party API Latency\n```python\n# Monitor external API latency\nimport requests\nimport time\n\ndef fetch_with_monitoring(url):\n    start = time.time()\n    response = requests.get(url, timeout=5)\n    latency = time.time() - start\n    \n    if latency > 2.0:\n        print(f\"WARNING: {url} took {latency:.2f}s\")\n    \n    return response\n```\n\n---\n\n### Bottleneck #3: Infrastructure\n\n**Symptoms**:\n- High CPU/memory/disk I/O sustained\n- Latency spikes correlated with resource exhaustion\n- \"Out of memory\" or \"connection refused\" errors\n\n**Investigation checklist**:\n\n#### A. CPU Bottleneck\n```bash\n# Monitor CPU during load test\ntop -p $(pgrep -f \"python app.py\")\n\n# Symptom: CPU at 95-100% sustained\n```\n\n**Diagnosis**: CPU-bound operations (embeddings, complex algorithms)\n\n**Fixes**:\n- **Vertical scale**: Increase CPU cores\n- **Horizontal scale**: Add more instances behind load balancer\n- **Optimize code**: Profile with `cProfile` to find hotspots\n- **Offload**: Use GPU for embeddings\n\n#### B. Memory Bottleneck\n```bash\n# Monitor memory usage\nps aux | grep python\n# Or use memory_profiler\n\nfrom memory_profiler import profile\n\n@profile\ndef process_documents(docs):\n    embeddings = [embed(doc) for doc in docs]  # Loads all into memory\n    return embeddings\n```\n\n**Diagnosis**: Memory leak or large object allocation\n\n**Fixes**:\n- **Streaming**: Process data in batches instead of all at once\n- **Connection pool limits**: Prevent unbounded growth\n- **Garbage collection**: Explicitly delete large objects\n- **Vertical scale**: Increase RAM (temporary fix)\n\n#### C. Connection Pool Exhaustion\n```\nError: QueuePool limit of size 5 overflow 10 reached, connection timed out\n```\n\n**Diagnosis**: Database connection pool too small for concurrent users\n\n**Fix**:\n```python\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\n    DATABASE_URL,\n    pool_size=20,        # Increase from default 5\n    max_overflow=40,     # Extra connections under load\n    pool_timeout=60,     # Wait before error\n    pool_recycle=3600,   # Recycle connections hourly\n    pool_pre_ping=True   # Verify connection before use\n)\n```\n\n**Rule of thumb**: `pool_size = expected_concurrent_users / 10`\n\n---\n\n### Real-World Bottleneck Example\n\n**Scenario**: RAG system breaks at 125 concurrent users\n\n**Symptoms**:\n- p95 latency: 2s → 8s\n- Error rate: 0% → 25%\n- CPU: 40% (not bottleneck)\n- Memory: 60% (not bottleneck)\n\n**Investigation**:\n```bash\n# Check logs during load test\ntail -f logs/app.log | grep ERROR\n\n# Output:\n# RateLimitError: Requests to the OpenAI API have exceeded rate limits\n```\n\n**Diagnosis**: OpenAI rate limit (500 RPM) reached at 125 users * 0.5 queries/sec = 62.5 QPS = 3750 QPM\n\n**Fixes applied**:\n1. Implement query result caching (Redis)\n2. Reduce redundant embedding calls (cache document embeddings)\n3. Implement retry logic with exponential backoff\n\n**Result after fixes**:\n- Capacity increased: 125 → 400 concurrent users\n- Cache hit rate: 75%\n- OpenAI calls reduced: 3750 QPM → 900 QPM\n- Cost savings: $200/month → $50/month",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Bottleneck diagnosis helper\ndef diagnose_bottleneck(cpu_pct, memory_pct, error_rate, error_type):\n    \"\"\"\n    Simple decision tree for bottleneck diagnosis.\n    \n    Args:\n        cpu_pct: CPU usage percentage (0-100)\n        memory_pct: Memory usage percentage (0-100)\n        error_rate: Error rate percentage (0-100)\n        error_type: Primary error type (str)\n    \"\"\"\n    print(\"=== Bottleneck Diagnosis ===\\n\")\n    print(f\"CPU: {cpu_pct}%\")\n    print(f\"Memory: {memory_pct}%\")\n    print(f\"Error Rate: {error_rate}%\")\n    print(f\"Error Type: {error_type}\\n\")\n    \n    if \"RateLimitError\" in error_type or \"429\" in error_type:\n        print(\"✅ DIAGNOSIS: External API Rate Limit\")\n        print(\"   FIX: Implement caching, reduce API calls, upgrade tier\")\n    \n    elif \"QueuePool\" in error_type or \"connection\" in error_type.lower():\n        print(\"✅ DIAGNOSIS: Connection Pool Exhaustion\")\n        print(\"   FIX: Increase pool_size, add connection pooling\")\n    \n    elif cpu_pct > 80:\n        print(\"✅ DIAGNOSIS: CPU Bottleneck\")\n        print(\"   FIX: Optimize algorithms, vertical/horizontal scaling\")\n    \n    elif memory_pct > 85:\n        print(\"✅ DIAGNOSIS: Memory Bottleneck\")\n        print(\"   FIX: Check for memory leaks, implement streaming, add RAM\")\n    \n    elif error_rate < 1:\n        print(\"✅ DIAGNOSIS: System Healthy\")\n        print(\"   No immediate action needed\")\n    \n    else:\n        print(\"⚠️  DIAGNOSIS: Unknown Issue\")\n        print(\"   Check application logs for specific errors\")\n\n# Example scenarios\nprint(\"Scenario 1: OpenAI Rate Limit Hit\")\ndiagnose_bottleneck(cpu_pct=45, memory_pct=60, error_rate=25, error_type=\"RateLimitError: 429\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\nprint(\"Scenario 2: Database Connection Pool\")\ndiagnose_bottleneck(cpu_pct=30, memory_pct=50, error_rate=15, error_type=\"QueuePool limit reached\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\nprint(\"Scenario 3: CPU Overload\")\ndiagnose_bottleneck(cpu_pct=95, memory_pct=40, error_rate=5, error_type=\"Timeout\")\n\n# Expected: Correct diagnosis for each scenario",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Scaling Playbook (cache, batch, HPA)\n\n### The Scaling Hierarchy\n\n**Before throwing money at infrastructure, optimize first:**\n\n```\n1. Measure (load test)\n   ↓\n2. Optimize code (fix N+1 queries, add indexes)     ← Often 10x gains\n   ↓\n3. Add caching (Redis, in-memory)                   ← 2-100x speedup\n   ↓\n4. Batch operations (embeddings, DB queries)        ← 5-10x efficiency\n   ↓\n5. Vertical scaling (bigger instance)               ← 2-4x capacity\n   ↓\n6. Horizontal scaling (more instances + LB)         ← Unlimited capacity\n   ↓\n7. Auto-scaling (HPA - Horizontal Pod Autoscaler)   ← Dynamic elasticity\n```\n\n**Rule**: Each step up costs more money and complexity. Exhaust cheaper options first.\n\n---\n\n### Strategy 1: Caching\n\n**Impact**: 10-100x latency reduction, 80-95% cost savings on API calls\n\n#### Query Result Caching\n```python\nimport redis\nimport hashlib\nimport json\n\nredis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n\ndef cached_query(query_text, top_k=5, ttl=3600):\n    # Generate cache key\n    cache_key = f\"query:{hashlib.md5(query_text.encode()).hexdigest()}:{top_k}\"\n    \n    # Check cache\n    cached = redis_client.get(cache_key)\n    if cached:\n        print(f\"✅ Cache HIT: {query_text[:30]}...\")\n        return json.loads(cached)\n    \n    # Cache miss - compute result\n    print(f\"❌ Cache MISS: {query_text[:30]}...\")\n    result = run_rag_query(query_text, top_k)  # Expensive operation\n    \n    # Store in cache with TTL\n    redis_client.set(cache_key, json.dumps(result), ex=ttl)\n    return result\n\n# Simulate RAG query\ndef run_rag_query(query, top_k):\n    import time\n    time.sleep(0.5)  # Simulate 500ms latency\n    return {\"answer\": \"Cached response\", \"sources\": [\"doc1\", \"doc2\"]}\n```\n\n**Metrics**:\n- Before caching: 1000 queries = 1000 API calls = $0.20\n- After caching (80% hit rate): 1000 queries = 200 API calls = $0.04\n- **Savings**: $0.16 per 1000 queries\n\n#### Embedding Caching\n```python\n# Cache document embeddings (persist across restarts)\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n    cache_key = f\"embed:{hashlib.md5(text.encode()).hexdigest()}\"\n    \n    cached = redis_client.get(cache_key)\n    if cached:\n        return json.loads(cached)\n    \n    # Expensive OpenAI call\n    embedding = openai.embeddings.create(input=text, model=model).data[0].embedding\n    redis_client.set(cache_key, json.dumps(embedding))  # No TTL (permanent)\n    return embedding\n```\n\n**Impact**: Eliminate redundant embedding calls (especially for static documents)\n\n---\n\n### Strategy 2: Batching\n\n**Impact**: 5-10x reduction in API overhead\n\n#### Embedding Batching\n```python\n# BAD: 10 API calls for 10 documents\nembeddings = []\nfor doc in documents:\n    embedding = openai.embeddings.create(input=doc, model=\"text-embedding-3-small\")\n    embeddings.append(embedding.data[0].embedding)\n\n# GOOD: 1 API call for 10 documents (up to 2048 texts per batch)\ntexts = [doc.content for doc in documents]\nresponse = openai.embeddings.create(input=texts, model=\"text-embedding-3-small\")\nembeddings = [item.embedding for item in response.data]\n```\n\n**Impact**: 10x reduction in network overhead, API quota usage\n\n#### Database Batching\n```python\n# BAD: N individual queries (N+1 problem)\nfor doc_id in document_ids:\n    doc = db.query(Document).filter_by(id=doc_id).first()\n    process(doc)\n\n# GOOD: 1 bulk query\ndocs = db.query(Document).filter(Document.id.in_(document_ids)).all()\nfor doc in docs:\n    process(doc)\n```\n\n---\n\n### Strategy 3: Horizontal Scaling\n\n**When**: Code is optimized, caching implemented, vertical scaling limit reached\n\n**Concept**: Run multiple instances behind a load balancer\n\n```\n                    ┌───────────────┐\nUser requests  →    │ Load Balancer │\n                    └───────┬───────┘\n                            │\n            ┌───────────────┼───────────────┐\n            ↓               ↓               ↓\n        ┌───────┐       ┌───────┐       ┌───────┐\n        │ App 1 │       │ App 2 │       │ App 3 │\n        └───┬───┘       └───┬───┘       └───┬───┘\n            │               │               │\n            └───────────────┴───────────────┘\n                            ↓\n                    ┌───────────────┐\n                    │   Database    │\n                    │  (shared)     │\n                    └───────────────┘\n```\n\n**Requirements**:\n1. **Stateless application**: No local session storage\n2. **Shared state**: Use Redis for sessions, cache\n3. **Health checks**: `/health` endpoint for load balancer\n4. **Sticky sessions OFF**: Load balancer distributes evenly\n\n**Example (Railway auto-scaling)**:\n```yaml\n# railway.json\n{\n  \"deploy\": {\n    \"numReplicas\": 3,\n    \"restartPolicyType\": \"ON_FAILURE\",\n    \"healthcheckPath\": \"/health\",\n    \"healthcheckTimeout\": 100\n  }\n}\n```\n\n---\n\n### Strategy 4: Vertical Scaling\n\n**When**: Single-instance performance needs boost (simpler than horizontal)\n\n**Trade-offs**:\n- **Pros**: No code changes, simpler deployment\n- **Cons**: Limited ceiling, single point of failure, expensive at high specs\n\n**Example progression**:\n1. Start: 1 CPU, 1GB RAM → 50 concurrent users\n2. Upgrade: 2 CPU, 4GB RAM → 100 concurrent users\n3. Upgrade: 4 CPU, 8GB RAM → 200 concurrent users\n4. **Hit ceiling**: 8 CPU, 16GB RAM → 400 users (then switch to horizontal)\n\n---\n\n### Strategy 5: Auto-scaling (HPA)\n\n**Horizontal Pod Autoscaler**: Automatically add/remove instances based on metrics\n\n**Trigger metrics**:\n- CPU usage >70% sustained for 5 minutes\n- Memory usage >80%\n- Custom metric: request queue depth >100\n\n**Example (Kubernetes HPA)**:\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: rag-api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: rag-api\n  minReplicas: 2           # Always maintain minimum\n  maxReplicas: 10          # Cost control limit\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 60    # Wait 1 min before scaling up\n    scaleDown:\n      stabilizationWindowSeconds: 300   # Wait 5 min before scaling down\n```\n\n**Railway/Render auto-scaling**:\n- Configure in dashboard: min/max replicas, CPU threshold\n- Simpler than Kubernetes, less control\n\n---\n\n### Scaling Decision Matrix\n\n| Symptom | Likely Cause | Solution | Cost | Complexity |\n|---------|--------------|----------|------|------------|\n| High p99, normal p50 | Outliers/cold starts | Horizontal scale | Medium | Medium |\n| High CPU (>80%) | CPU-bound work | Vertical scale OR optimize code | High | Low/High |\n| High memory (>85%) | Memory leak | Fix code, then vertical scale | Medium | High |\n| DB queries slow | Missing indexes | Add indexes (free!) | Free | Low |\n| OpenAI 429 errors | Rate limits | Caching, batching, upgrade tier | Low-High | Medium |\n| Inconsistent latency | No caching | Implement Redis cache | Low | Medium |\n\n---\n\n### Real-World Scaling Example\n\n**Starting point**: 1 instance, 2 CPU, 4GB RAM, no caching\n\n**Load test results**:\n- Capacity: 50 concurrent users\n- p95 latency: 2.5s\n- Bottleneck: OpenAI rate limits + repeated queries\n\n**Optimization journey**:\n\n1. **Add query caching (Redis)**  \n   - Cost: $10/month (Redis hosting)  \n   - Result: Capacity → 150 users (80% cache hit rate)  \n   - Time: 4 hours implementation\n\n2. **Implement embedding batching**  \n   - Cost: Free (code change)  \n   - Result: API calls reduced by 60%  \n   - Time: 2 hours implementation\n\n3. **Vertical scale: 4 CPU, 8GB RAM**  \n   - Cost: +$40/month  \n   - Result: Capacity → 300 users  \n   - Time: 5 minutes (infrastructure change)\n\n4. **Horizontal scale: 3 instances + load balancer**  \n   - Cost: +$80/month (3x instances)  \n   - Result: Capacity → 900 users  \n   - Time: 1 hour (setup LB, test)\n\n**Total investment**:\n- Cost: +$130/month\n- Time: ~7 hours\n- Capacity gain: 50 → 900 users (18x improvement)\n- ROI: Supports 10x growth without re-architecting",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simulate caching impact on performance\nimport time\nimport random\n\n# Simple in-memory cache for demonstration\ncache = {}\n\ndef expensive_operation(query_id):\n    \"\"\"Simulate expensive RAG query (500ms)\"\"\"\n    time.sleep(0.05)  # Reduced for notebook demo (50ms instead of 500ms)\n    return f\"Result for query {query_id}\"\n\ndef cached_operation(query_id):\n    \"\"\"Cached version of expensive operation\"\"\"\n    if query_id in cache:\n        return cache[query_id], True  # Cache hit\n    \n    result = expensive_operation(query_id)\n    cache[query_id] = result\n    return result, False  # Cache miss\n\n# Simulate 100 queries with some repetition (realistic pattern)\nqueries = [random.randint(1, 20) for _ in range(100)]  # 20 unique queries, 100 total\n\nprint(\"Running 100 queries without caching...\")\nstart = time.time()\nfor q in queries:\n    expensive_operation(q)\nno_cache_time = time.time() - start\n\nprint(f\"Time without cache: {no_cache_time:.2f}s\\n\")\n\n# Reset cache\ncache.clear()\n\nprint(\"Running 100 queries WITH caching...\")\nstart = time.time()\nhits = 0\nmisses = 0\nfor q in queries:\n    result, is_hit = cached_operation(q)\n    if is_hit:\n        hits += 1\n    else:\n        misses += 1\ncache_time = time.time() - start\n\nprint(f\"Time with cache: {cache_time:.2f}s\")\nprint(f\"Cache hits: {hits}\")\nprint(f\"Cache misses: {misses}\")\nprint(f\"Hit rate: {(hits/100)*100:.1f}%\")\nprint(f\"Speedup: {no_cache_time/cache_time:.1f}x\")\n\n# Expected:\n# - Significant speedup (3-5x)\n# - High cache hit rate (70-80%)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: When NOT to Load Test\n\n### The Cost-Benefit Reality\n\n**Load testing is powerful, but not always the right tool.**\n\nThis section helps you decide when to invest in load testing vs. other priorities.\n\n---\n\n### When to SKIP Load Testing\n\n#### 1. Small Scale, No Growth Plans\n**Scenario**: Internal tool with 10-50 daily users, stable usage\n\n**Why skip**:\n- Setup cost: 8-12 hours\n- Maintenance: 2-4 hours per sprint\n- Benefit: Minimal (system unlikely to reach capacity limits)\n\n**Better approach**: Monitor basic metrics (response time, error rate), optimize if issues arise\n\n---\n\n#### 2. Staging Environment ≠ Production\n**Scenario**: Test environment has different data size, network, or configuration\n\n**Examples of mismatch**:\n- Staging: 1GB test data → Production: 100GB real data\n- Staging: Single region → Production: Global CDN\n- Staging: Mock external APIs → Production: Real APIs with rate limits\n\n**Why skip**: Load test results won't predict production behavior\n\n**Better approach**: \n- Invest in production monitoring (APM tools)\n- Conduct small-scale production canary tests\n- Use production database replicas for realistic testing\n\n---\n\n#### 3. MVP / Early-Stage Projects\n**Scenario**: Building product with uncertain growth, architecture may pivot\n\n**Why skip**: Premature optimization wastes time on code that may be rewritten\n\n**Better approach**:\n- Optimize obvious bottlenecks (N+1 queries, missing database indexes)\n- Monitor production metrics to identify real user patterns\n- Load test after achieving product-market fit and stable architecture\n\n---\n\n#### 4. Limited Failure Modes Covered\n**Scenario**: Expecting comprehensive system validation\n\n**Reality check**: Load tests reveal **capacity** limits, not all failure modes\n\n**What load tests DON'T catch**:\n- Security vulnerabilities (use penetration testing)\n- Data corruption (use integration tests)\n- Edge cases in business logic (use unit/functional tests)\n- Network partition failures (use chaos engineering)\n- Authentication/authorization bugs (use security testing)\n\n**Takeaway**: Load testing complements, doesn't replace, other testing types\n\n---\n\n### When to USE Load Testing\n\n#### 1. Growth Trajectory\n**Scenario**: User base growing 20%+ monthly, approaching capacity limits\n\n**Signs you need load testing**:\n- CPU usage trending upward (currently 50%, will hit 80% in 2 months)\n- Response times increasing week-over-week\n- Planning to onboard large customer (10x typical usage)\n\n**Value**: Proactively identify scaling needs before outages\n\n---\n\n#### 2. SLA Requirements\n**Scenario**: Performance guarantees in contracts\n\n**Examples**:\n- E-commerce: \"p95 latency <1s during checkout\"\n- API provider: \"99.9% uptime, p99 <2s\"\n- Enterprise customer: \"Support 1000 concurrent users\"\n\n**Value**: Validate compliance, avoid SLA penalties, maintain reputation\n\n---\n\n#### 3. Infrastructure Investment Decisions\n**Scenario**: Evaluating whether to upgrade hosting plan\n\n**Example decision**:\n- Current plan: $40/month, 2 CPU, 4GB RAM\n- Upgrade plan: $120/month, 8 CPU, 16GB RAM\n- Question: \"Will upgrade support expected growth?\"\n\n**Load testing provides**:\n- Current capacity: 100 concurrent users\n- Projected capacity after upgrade: 400 concurrent users\n- ROI calculation: $80/month for 3x capacity\n\n**Value**: Data-driven infrastructure spending (avoid over/under-provisioning)\n\n---\n\n#### 4. Pre-Launch Validation\n**Scenario**: Launching new feature, marketing campaign, or product\n\n**Examples**:\n- Product Hunt launch (expect 10x typical traffic spike)\n- Black Friday sale (high concurrency)\n- New API endpoint (unknown performance characteristics)\n\n**Value**: Confidence that system handles expected load, identify bottlenecks before public exposure\n\n---\n\n### Decision Framework\n\nAsk these questions:\n\n1. **Do I have a growth trajectory?**  \n   - No → Skip load testing, monitor production  \n   - Yes → Continue\n\n2. **Does my staging environment match production?**  \n   - No → Fix environment or use production testing  \n   - Yes → Continue\n\n3. **Do I have SLA requirements or revenue risk?**  \n   - No → Low priority, consider deferring  \n   - Yes → High priority, load test now\n\n4. **Am I making infrastructure decisions soon?**  \n   - No → Monitor, test later when needed  \n   - Yes → Load test to inform decision\n\n5. **Is my architecture stable?**  \n   - No → Optimize obvious issues first  \n   - Yes → Load test to find limits\n\n---\n\n### Alternative Approaches\n\nIf load testing doesn't fit, consider:\n\n**1. Production Monitoring (APM)**\n- Tools: DataDog, New Relic, Sentry\n- Real user metrics (RUM) capture actual performance\n- Cost: $0-100/month\n- Effort: 1-2 hours setup\n\n**2. Canary Deployments**\n- Roll out changes to 5% of users first\n- Monitor error rates, latency before full deployment\n- Built into many CI/CD platforms\n\n**3. Feature Flags**\n- Enable new features for subset of users\n- Gradually increase percentage while monitoring\n- Tools: LaunchDarkly, Unleash\n\n**4. Basic Performance Testing**\n- Run 10-20 concurrent requests manually\n- Check for obvious errors, slow queries\n- Effort: 30 minutes, no specialized tools\n\n---\n\n### Cost-Benefit Summary\n\n| Scenario | Time Investment | Load Test? | Alternative |\n|----------|-----------------|------------|-------------|\n| 50 daily users, stable | 8-12 hours | ❌ No | Basic monitoring |\n| 1000 daily users, 20% growth | 8-12 hours | ✅ Yes | N/A |\n| MVP, uncertain architecture | 8-12 hours | ❌ No | Optimize obvious issues |\n| SLA: p95 <1s guaranteed | 8-12 hours | ✅ Yes | N/A |\n| Staging ≠ Production | 8-12 hours | ❌ No | Production canary testing |\n| Pre-launch (expecting spike) | 8-12 hours | ✅ Yes | N/A |\n\n---\n\n### Final Takeaway\n\n**Load testing is a tool, not a requirement.**\n\n**Golden rules**:\n1. **Measure real impact**: If you can't quantify benefits, defer\n2. **Optimize first**: Fix N+1 queries, add indexes before scaling infrastructure\n3. **Match environment**: Only test if staging resembles production\n4. **Prioritize**: SLAs and growth justify load testing; curiosity doesn't\n\n**When in doubt**: Start with basic monitoring, load test when evidence shows it's needed.\n\n---\n\n## Summary: M3.4 Complete\n\nYou've learned:\n1. ✅ When to load test (and when to skip)\n2. ✅ 5 test types: smoke, load, stress, spike, soak\n3. ✅ Interpreting p50/p95/p99 metrics\n4. ✅ Diagnosing bottlenecks: code, external, infrastructure\n5. ✅ Scaling playbook: cache, batch, vertical, horizontal, auto-scale\n\n**Next steps**:\n- Run `locustfile.py` against your own API\n- Analyze results using techniques from Section 3\n- Optimize based on bottlenecks identified\n- Scale strategically using Section 5 playbook\n\n**Resources**:\n- `locustfile.py` - Complete load testing implementation\n- `scaling_notes.md` - Detailed scaling reference\n- `README.md` - Quick start guide\n- `requirements.txt` - Dependencies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Decision helper: Should you load test?\ndef should_load_test(daily_users, growth_rate_monthly, has_sla, staging_matches_prod, architecture_stable):\n    \"\"\"\n    Decision framework for load testing.\n    \n    Args:\n        daily_users: Number of daily active users\n        growth_rate_monthly: Growth rate as percentage (e.g., 20 for 20%)\n        has_sla: Boolean - do you have SLA requirements?\n        staging_matches_prod: Boolean - does staging match production environment?\n        architecture_stable: Boolean - is architecture stable (not MVP)?\n    \n    Returns:\n        Decision and reasoning\n    \"\"\"\n    score = 0\n    reasons = []\n    \n    # Scoring criteria\n    if daily_users > 500:\n        score += 2\n        reasons.append(\"✅ High user base (>500 daily users)\")\n    elif daily_users > 100:\n        score += 1\n        reasons.append(\"⚠️  Moderate user base (100-500 users)\")\n    else:\n        reasons.append(\"❌ Low user base (<100 users)\")\n    \n    if growth_rate_monthly > 15:\n        score += 2\n        reasons.append(\"✅ High growth (>15% monthly)\")\n    elif growth_rate_monthly > 5:\n        score += 1\n        reasons.append(\"⚠️  Moderate growth (5-15% monthly)\")\n    else:\n        reasons.append(\"❌ Low/no growth (<5% monthly)\")\n    \n    if has_sla:\n        score += 3\n        reasons.append(\"✅ SLA requirements (critical)\")\n    \n    if not staging_matches_prod:\n        score -= 2\n        reasons.append(\"❌ Staging ≠ Production (results unreliable)\")\n    \n    if not architecture_stable:\n        score -= 1\n        reasons.append(\"⚠️  Architecture unstable (may change)\")\n    \n    # Decision\n    print(\"=== Load Testing Decision Framework ===\\n\")\n    for reason in reasons:\n        print(reason)\n    \n    print(f\"\\nScore: {score}/7\")\n    \n    if score >= 5:\n        print(\"\\n✅ RECOMMENDATION: YES - Load test now\")\n        print(\"   Priority: HIGH\")\n    elif score >= 3:\n        print(\"\\n⚠️  RECOMMENDATION: CONSIDER - Load test if planning infrastructure changes\")\n        print(\"   Priority: MEDIUM\")\n    else:\n        print(\"\\n❌ RECOMMENDATION: SKIP - Focus on monitoring and optimization\")\n        print(\"   Priority: LOW\")\n    \n    return score\n\n# Example scenarios\nprint(\"Scenario 1: Early-stage startup\")\nshould_load_test(daily_users=50, growth_rate_monthly=5, has_sla=False, \n                 staging_matches_prod=False, architecture_stable=False)\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\nprint(\"Scenario 2: Growing SaaS product\")\nshould_load_test(daily_users=1000, growth_rate_monthly=20, has_sla=True, \n                 staging_matches_prod=True, architecture_stable=True)\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\nprint(\"Scenario 3: Established product with moderate growth\")\nshould_load_test(daily_users=500, growth_rate_monthly=10, has_sla=False, \n                 staging_matches_prod=True, architecture_stable=True)\n\n# Expected: Different recommendations based on criteria",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}